
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://blevinscm.github.com/genai-docs/open-models/use-hex-llm/">
      
      
      
      
      <link rel="icon" href="../../assets/google-cloud-vertex-ai.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Serve Open Models Using Hex Llm Premium Container On Cloud Tpustay Organized With Collectionssave An - Vertex Generative AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Google Sans";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../js/chat-widget/chat-widget.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#serve-open-models-using-hex-llm-premium-container-on-cloud-tpu" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Vertex Generative AI" class="md-header__button md-logo" aria-label="Vertex Generative AI" data-md-component="logo">
      
  <img src="../../assets/google-cloud-vertex-ai.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Vertex Generative AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Serve Open Models Using Hex Llm Premium Container On Cloud Tpustay Organized With Collectionssave An
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/blevinscm/genai-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Generative-AI-on-Vertex-AI-Cookbook/" class="md-tabs__link">
          
  
  
    
  
  Learn

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../agent-engine/Set-up-the-environment/" class="md-tabs__link">
          
  
  
    
  
  Build

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../models/Introduction-to-tuning/" class="md-tabs__link">
          
  
  
    
  
  Tune

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../deploy/Deploy-generative-AI-models/" class="md-tabs__link">
          
  
  
    
  
  Deploy

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../agent-engine/manage/Manage-deployed-agents/" class="md-tabs__link">
          
  
  
    
  
  Manage

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../models/evaluation-overview/" class="md-tabs__link">
          
  
  
    
  
  Optimize

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Vertex Generative AI" class="md-nav__button md-logo" aria-label="Vertex Generative AI" data-md-component="logo">
      
  <img src="../../assets/google-cloud-vertex-ai.png" alt="logo">

    </a>
    Vertex Generative AI
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/blevinscm/genai-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Learn
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Learn
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Generative-AI-on-Vertex-AI-Cookbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vertex AI Generative AI Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Generative-AI-glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Generative-AI-on-Vertex-AI-release-notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Release Notes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supported-models_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supported Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model-reference/gemini/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gemini Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image/Imagen-on-Vertex-AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Imagen Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/code-models-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Code Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intelligent-code-commenter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Intelligent Code Commenter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Use-Gemma-open-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Open Models (Gemma, Llama)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../partner-models/use-partner-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Partner Models (Claude, AI21)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../learn/prompts/Introduction-to-prompting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction to Prompting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../learn/prompts/Overview-of-prompting-strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompting Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multimodal/Design-multimodal-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Concepts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../embeddings/Choose-an-embeddings-task-type/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multimodal/Introduction-to-function-calling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Function Calling Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../grounding/Ground-responses-using-RAG/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Grounding Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-builder/Vertex-AI-Agent-Builder-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Builder Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/Vertex-AI-Agent-Engine-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Engine Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rag-engine/Vertex-AI-RAG-Engine-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Engine Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../context-cache/Context-caching-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Context Caching Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../extensions/Extensions-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Extensions Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../example-store/Example-Store-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Example Store Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Generative-AI-and-data-governance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Governance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Security-controls-for-Generative-AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Security & Safety
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deprecations/Model-versions-and-lifecycle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deprecations & Lifecycle
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-help_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Help
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Build
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Build
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/Set-up-the-environment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Set up Environment (Agent Engine)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/develop/Develop-a-LangChain-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Develop LangChain Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/develop/Develop-an-Agent-Development-Kit-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Develop ADK Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/use/Use-a-LangChain-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use LangChain Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/use/Use-a-LangGraph-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use LangGraph Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chat/Design-chat-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Design Chat Prompts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multimodal/Text-generationbookmark_borderbookmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Text with Gemini
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image/Generate-images-using-text-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Images with Imagen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Generate-images-with-Gemini/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Images with Gemini
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../video/generate-videos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Video with Veo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../embeddings/Get-batch-text-embeddings-predictions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Get Text Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../embeddings/Get-multimodal-embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Get Multimodal Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model-reference/Function-calling-reference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Function Calling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../grounding/Use-Google-Search-suggestions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Grounding (Google Search)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../grounding/Grounding-with-your-data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Grounding (Your Data)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rag-engine/Use-data-ingestion-with-Vertex-AI-RAG-Engine/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Build with RAG Engine (Data Ingestion)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rag-engine/use-vertexai-vector-search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Build with RAG Engine (Vector Search)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Use-a-Weaviate-database-with-Vertex-AI-RAG-Engine/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Build with RAG Engine (Weaviate)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../context-cache/Create-a-context-cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create Context Cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../extensions/Create-and-run-extensions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create Extensions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../example-store/Retrieve-examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Example Store
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multimodal/audio-understanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Audio Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multimodal/Document-understanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Document Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../multimodal/Video-understanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Video Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../prompt-gallery/samples/summarize_summarize_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Gallery Samples
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tune
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Tune
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/Introduction-to-tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction to Model Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/About-supervised-fine-tuning-for-Gemini-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Gemini Models (Supervised)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gemini-supervised-tuning-prepare/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prepare Data for Gemini SFT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/tune_gemini/text_tune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Gemini for Text
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image/Style-customization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Imagen Models (Style/Subject)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/Tune-function-calling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Function Calling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model-garden/LoRA-and-QLoRA-recommendations-for-LLMs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA & QLoRA Recommendations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Fine-tune-RAG-transformations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tune RAG Transformations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deploy
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Deploy
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/Deploy-generative-AI-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deploy Generative AI Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/deploy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deploy Agent (Agent Engine)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/Deployments-and-endpoints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deployments and Endpoints
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model-garden/Use-models-in-Model-Garden/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Models from Model Garden
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Purchase-Provisioned-Throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Purchase Provisioned Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Use-Provisioned-Throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Provisioned Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../migrate/Migrate-to-the-Gemini-API-from-Azure-OpenAI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Migrate from Azure OpenAI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../migrate/openai/Using-OpenAI-libraries-with-Vertex-AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use OpenAI Libraries with Vertex AI
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Manage
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Manage
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/manage/Manage-deployed-agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Deployed Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/manage/logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/manage/monitoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/sessions/Manage-sessions-using-direct-API-calls/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Agent Sessions (API)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/sessions/Manage-sessions-with-Agent-Development-Kit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Agent Sessions (ADK)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../context-cache/Get-information-about-a-context-cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Context Cache (Get/Delete)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Manage-your-RAG-knowledge-base-corpus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage RAG Corpus
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../learn/Model-monitoring-metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Monitoring Metrics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quotas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quotas & Limits
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/troubleshooting/Troubleshoot-deploying-an-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshoot Agent Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent-engine/troubleshooting/Troubleshoot-developing-an-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshoot Agent Development
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimize
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Optimize
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/evaluation-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Evaluation Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/Gen-AI-evaluation-service-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gen AI Evaluation Service
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/Run-AutoSxS-pipeline-to-perform-pairwise-model-based-evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Run AutoSxS (Pairwise Evaluation)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/Evaluate-Gen-AI-agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluate Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../learn/prompts/Optimize-promptsbookmark_borderbookmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimize Prompts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Retrieval-and-ranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Retrieval and Ranking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model-reference/Vertex-AI-Model-Optimizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vertex AI Model Optimizer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../provisioned-throughput/Calculate-Provisioned-Throughput-requirements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calculate Provisioned Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model-reference/count-tokens/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Count Tokens
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../context-cache/Context-caching-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Context Caching Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    <span class="md-ellipsis">
      Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#advanced-features" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multi-host-serving" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-host serving
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disaggregated-serving-experimental" class="md-nav__link">
    <span class="md-ellipsis">
      Disaggregated serving [experimental]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prefix-caching" class="md-nav__link">
    <span class="md-ellipsis">
      Prefix caching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-bit-quantization-support" class="md-nav__link">
    <span class="md-ellipsis">
      4-bit quantization support
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#get-started-in-model-garden" class="md-nav__link">
    <span class="md-ellipsis">
      Get started in Model Garden
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Get started in Model Garden">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-playground" class="md-nav__link">
    <span class="md-ellipsis">
      Use playground
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-one-click-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      Use one-click deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-the-colab-enterprise-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      Use the Colab Enterprise notebook
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-server-arguments-and-environment-variables" class="md-nav__link">
    <span class="md-ellipsis">
      Configure server arguments and environment variables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Configure server arguments and environment variables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tune-server-arguments" class="md-nav__link">
    <span class="md-ellipsis">
      Tune server arguments
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#request-cloud-tpu-quota" class="md-nav__link">
    <span class="md-ellipsis">
      Request Cloud TPU quota
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/blevinscm/genai-docs/edit/main/docs/open-models/use-hex-llm.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="serve-open-models-using-hex-llm-premium-container-on-cloud-tpu">Serve open models using Hex-LLM premium container on Cloud TPU<a class="headerlink" href="#serve-open-models-using-hex-llm-premium-container-on-cloud-tpu" title="Permanent link">&para;</a></h1>
<p>Hex-LLM, a high-efficiency large language model (LLM) serving with XLA, is the
Vertex AI LLM serving framework that's designed and optimized for <a href="../../multimodal/Content-generation-parameters/">Cloud TPU</a> hardware. Hex-LLM combines LLM serving technologies such as
continuous batching and <a href="https://arxiv.org/abs/2309.06180">PagedAttention</a> with
Vertex AI optimizations that are tailored for
<a href="https://openxla.org/xla">XLA</a> and Cloud TPU. It's a high-efficiency
and low-cost LLM serving on Cloud TPU for open source models.</p>
<p>Hex-LLM is available in
<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models">Model Garden</a> through model
playground, one-click deployment, and notebook.</p>
<h2 id="features">Features<a class="headerlink" href="#features" title="Permanent link">&para;</a></h2>
<p>Hex-LLM is based on open source projects with Google's own optimizations for XLA
and Cloud TPU. Hex-LLM achieves high throughput and low latency when serving
frequently used LLMs.</p>
<p>Hex-LLM includes the following optimizations:</p>
<ul>
<li>Token-based continuous batching algorithm to help ensure models are fully
 utilizing the hardware with a large number of concurrent requests.</li>
<li>A complete rewrite of the attention kernels that are optimized for XLA.</li>
<li>Flexible and composable data parallelism and tensor parallelism strategies
 with highly optimized weight sharding methods to efficiently run LLMs on
 multiple Cloud TPU chips.</li>
</ul>
<p>Hex-LLM supports a wide range of dense and sparse LLMs:</p>
<ul>
<li>Gemma 2B and 7B</li>
<li>Gemma 2 9B and 27B</li>
<li>Llama 2 7B, 13B and 70B</li>
<li>Llama 3 8B and 70B</li>
<li>Llama 3.1 8B and 70B</li>
<li>Llama 3.2 1B and 3B</li>
<li>Llama Guard 3 1B and 8B</li>
<li>Mistral 7B</li>
<li>Mixtral 8x7B and 8x22B</li>
<li>Phi-3 mini and medium</li>
<li>Qwen2 0.5B, 1.5B and 7B</li>
<li>Qwen2.5 0.5B, 1.5B, 7B, 14B and 32B AWQ</li>
</ul>
<p><strong>Note:</strong> Hex-LLM can serve the 70B models in full precision or quantized
(int8, int4) precision.</p>
<p>Hex-LLM also provides a variety of features, such as the following:</p>
<ul>
<li>Hex-LLM is included in a single container. Hex-LLM packages the API server,
 inference engine, and supported models into a single Docker image to be
 deployed.</li>
<li>Compatible with the <a href="https://huggingface.co/models">Hugging Face models</a>
 format. Hex-LLM can load a Hugging Face model from local disk, the Hugging
 Face Hub, and a Cloud Storage bucket.</li>
<li>Quantization using
 <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> and
 <a href="https://github.com/mit-han-lab/llm-awq">AWQ</a>.</li>
<li>Dynamic <a href="https://arxiv.org/abs/2106.09685">LoRA</a> loading. Hex-LLM is able to
 load the LoRA weights through reading the request argument during serving.</li>
</ul>
<h3 id="advanced-features">Advanced features<a class="headerlink" href="#advanced-features" title="Permanent link">&para;</a></h3>
<p>Hex-LLM supports the following advanced features:</p>
<ul>
<li>Multi-host serving</li>
<li>Disaggregated serving [experimental]</li>
<li>Prefix caching</li>
<li>4-bit quantization support</li>
</ul>
<h4 id="multi-host-serving">Multi-host serving<a class="headerlink" href="#multi-host-serving" title="Permanent link">&para;</a></h4>
<p>Hex-LLM now supports serving models with a <a href="https://cloud.google.com/tpu/docs/tpus-in-gke#multi-host">multi-host TPU slice</a>.
This feature lets you serve large models that can't be loaded
into a single host TPU VM, which contains at most eight v5e cores.</p>
<p>To enable this feature, set <code>--num_hosts</code> in the Hex-LLM container arguments and
set <code>--tpu_topology</code> in the Vertex AI SDK model upload request. The
following example shows how to deploy the Hex-LLM container with a TPU 4x4 v5e
topology that serves the Llama 3.1 70B bfloat16 model:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">hexllm_args</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a> <span class="s2">&quot;--host=0.0.0.0&quot;</span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a> <span class="s2">&quot;--port=7080&quot;</span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a> <span class="s2">&quot;--model=meta-llama/Meta-Llama-3.1-70B&quot;</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a> <span class="s2">&quot;--data_parallel_size=1&quot;</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a> <span class="s2">&quot;--tensor_parallel_size=16&quot;</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a> <span class="s2">&quot;--num_hosts=4&quot;</span><span class="p">,</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a> <span class="s2">&quot;--hbm_utilization_factor=0.9&quot;</span><span class="p">,</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="p">]</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a> <span class="n">display_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a> <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">HEXLLM_DOCKER_URI</span><span class="p">,</span>
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a> <span class="n">serving_container_command</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;hex_llm.server.api_server&quot;</span><span class="p">],</span>
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a> <span class="n">serving_container_args</span><span class="o">=</span><span class="n">hexllm_args</span><span class="p">,</span>
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a> <span class="n">serving_container_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">7080</span><span class="p">],</span>
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a> <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="s2">&quot;/generate&quot;</span><span class="p">,</span>
</span><span id="__span-0-18"><a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a> <span class="n">serving_container_health_route</span><span class="o">=</span><span class="s2">&quot;/ping&quot;</span><span class="p">,</span>
</span><span id="__span-0-19"><a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a> <span class="n">serving_container_environment_variables</span><span class="o">=</span><span class="n">env_vars</span><span class="p">,</span>
</span><span id="__span-0-20"><a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a> <span class="n">serving_container_shared_memory_size_mb</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">),</span> <span class="c1"># 16 GB</span>
</span><span id="__span-0-21"><a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a> <span class="n">serving_container_deployment_timeout</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
</span><span id="__span-0-22"><a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a> <span class="n">location</span><span class="o">=</span><span class="n">TPU_DEPLOYMENT_REGION</span><span class="p">,</span>
</span><span id="__span-0-23"><a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="p">)</span>
</span><span id="__span-0-24"><a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>
</span><span id="__span-0-25"><a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-0-26"><a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a> <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
</span><span id="__span-0-27"><a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-0-28"><a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a> <span class="n">tpu_topology</span><span class="o">=</span><span class="s2">&quot;4x4&quot;</span><span class="p">,</span>
</span><span id="__span-0-29"><a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a> <span class="n">deploy_request_timeout</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span>
</span><span id="__span-0-30"><a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">service_account</span><span class="p">,</span>
</span><span id="__span-0-31"><a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a> <span class="n">min_replica_count</span><span class="o">=</span><span class="n">min_replica_count</span><span class="p">,</span>
</span><span id="__span-0-32"><a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a> <span class="n">max_replica_count</span><span class="o">=</span><span class="n">max_replica_count</span><span class="p">,</span>
</span><span id="__span-0-33"><a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a><span class="p">)</span>
</span></code></pre></div>
<p>For an end-to-end tutorial for deploying the Hex-LLM container with a multi-host
TPU topology, see the <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_1_deployment.ipynb">Vertex AI Model Garden - Llama 3.1 (Deployment) notebook</a>.</p>
<p>In general, the only changes needed to enable multi-host serving are:</p>
<ol>
<li>Set argument <code>--tensor_parallel_size</code> to the total number of cores within the
 TPU topology.</li>
<li>Set argument <code>--num_hosts</code> to the number of hosts within the TPU topology.</li>
<li>Set <code>--tpu_topology</code> with the Vertex AI SDK model upload API.</li>
</ol>
<h4 id="disaggregated-serving-experimental">Disaggregated serving [experimental]<a class="headerlink" href="#disaggregated-serving-experimental" title="Permanent link">&para;</a></h4>
<p>Hex-LLM now supports disaggregated serving as an experimental feature. It can
only be enabled on the single host setup and the performance is under tuning.</p>
<p>Disaggregated serving is an effective method for balancing Time to First Token
(TTFT) and Time Per Output Token (TPOT) for each request, and the overall
serving throughput. It separates the prefill phase and the decode phase into
different workloads so that they don't interfere with each other. This method
is especially useful for scenarios that set strict latency requirements.</p>
<p>To enable this feature, set <code>--disagg_topo</code> in the Hex-LLM container arguments.
The following is an example that shows how to deploy the Hex-LLM container on
TPU v5e-8 that serves the Llama 3.1 8B bfloat16 model:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">hexllm_args</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a> <span class="s2">&quot;--host=0.0.0.0&quot;</span><span class="p">,</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a> <span class="s2">&quot;--port=7080&quot;</span><span class="p">,</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a> <span class="s2">&quot;--model=meta-llama/Llama-3.1-8B&quot;</span><span class="p">,</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a> <span class="s2">&quot;--data_parallel_size=1&quot;</span><span class="p">,</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a> <span class="s2">&quot;--tensor_parallel_size=2&quot;</span><span class="p">,</span>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a> <span class="s2">&quot;--disagg_topo=3,1&quot;</span><span class="p">,</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a> <span class="s2">&quot;--hbm_utilization_factor=0.9&quot;</span><span class="p">,</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="p">]</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a> <span class="n">display_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a> <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">HEXLLM_DOCKER_URI</span><span class="p">,</span>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a> <span class="n">serving_container_command</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;hex_llm.server.api_server&quot;</span><span class="p">],</span>
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a> <span class="n">serving_container_args</span><span class="o">=</span><span class="n">hexllm_args</span><span class="p">,</span>
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a> <span class="n">serving_container_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">7080</span><span class="p">],</span>
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a> <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="s2">&quot;/generate&quot;</span><span class="p">,</span>
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a> <span class="n">serving_container_health_route</span><span class="o">=</span><span class="s2">&quot;/ping&quot;</span><span class="p">,</span>
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a> <span class="n">serving_container_environment_variables</span><span class="o">=</span><span class="n">env_vars</span><span class="p">,</span>
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a> <span class="n">serving_container_shared_memory_size_mb</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">),</span> <span class="c1"># 16 GB</span>
</span><span id="__span-1-21"><a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a> <span class="n">serving_container_deployment_timeout</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
</span><span id="__span-1-22"><a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a> <span class="n">location</span><span class="o">=</span><span class="n">TPU_DEPLOYMENT_REGION</span><span class="p">,</span>
</span><span id="__span-1-23"><a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a><span class="p">)</span>
</span><span id="__span-1-24"><a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>
</span><span id="__span-1-25"><a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a><span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-1-26"><a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a> <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
</span><span id="__span-1-27"><a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-1-28"><a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a> <span class="n">deploy_request_timeout</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span>
</span><span id="__span-1-29"><a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">service_account</span><span class="p">,</span>
</span><span id="__span-1-30"><a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a> <span class="n">min_replica_count</span><span class="o">=</span><span class="n">min_replica_count</span><span class="p">,</span>
</span><span id="__span-1-31"><a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a> <span class="n">max_replica_count</span><span class="o">=</span><span class="n">max_replica_count</span><span class="p">,</span>
</span><span id="__span-1-32"><a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a><span class="p">)</span>
</span></code></pre></div>
<p>The <code>--disagg_topo</code> argument accepts a string in the format <code>"number_of_prefill_workers,number_of_decode_workers"</code>.
In the earlier example, it is set to <code>"3,1"</code> to configure three prefill workers
and 1 decode worker. Each worker uses two TPU v5e cores.</p>
<h4 id="prefix-caching">Prefix caching<a class="headerlink" href="#prefix-caching" title="Permanent link">&para;</a></h4>
<p>Prefix caching reduces Time to First Token (TTFT) for prompts that have
identical content at the beginning of the prompt, such as company-wide preambles,
common system instructions, and multi-turn conversation history. Instead of
processing the same input tokens repeatedly, Hex-LLM can retain a temporary
cache of the processed input token computations to improve TTFT.</p>
<p>To enable this feature, set <code>--enable_prefix_cache_hbm</code> in the Hex-LLM container
arguments. The following is an example that shows how to deploy the Hex-LLM
container on TPU v5e-8 that serves the Llama 3.1 8B bfloat16 model:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="n">hexllm_args</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a> <span class="s2">&quot;--host=0.0.0.0&quot;</span><span class="p">,</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a> <span class="s2">&quot;--port=7080&quot;</span><span class="p">,</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a> <span class="s2">&quot;--model=meta-llama/Llama-3.1-8B&quot;</span><span class="p">,</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a> <span class="s2">&quot;--data_parallel_size=1&quot;</span><span class="p">,</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a> <span class="s2">&quot;--tensor_parallel_size=2&quot;</span><span class="p">,</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a> <span class="s2">&quot;--hbm_utilization_factor=0.9&quot;</span><span class="p">,</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a> <span class="s2">&quot;--enable_prefix_cache_hbm&quot;</span><span class="p">,</span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="p">]</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a> <span class="n">display_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a> <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">HEXLLM_DOCKER_URI</span><span class="p">,</span>
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a> <span class="n">serving_container_command</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;hex_llm.server.api_server&quot;</span><span class="p">],</span>
</span><span id="__span-2-15"><a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a> <span class="n">serving_container_args</span><span class="o">=</span><span class="n">hexllm_args</span><span class="p">,</span>
</span><span id="__span-2-16"><a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a> <span class="n">serving_container_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">7080</span><span class="p">],</span>
</span><span id="__span-2-17"><a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a> <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="s2">&quot;/generate&quot;</span><span class="p">,</span>
</span><span id="__span-2-18"><a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a> <span class="n">serving_container_health_route</span><span class="o">=</span><span class="s2">&quot;/ping&quot;</span><span class="p">,</span>
</span><span id="__span-2-19"><a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a> <span class="n">serving_container_environment_variables</span><span class="o">=</span><span class="n">env_vars</span><span class="p">,</span>
</span><span id="__span-2-20"><a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a> <span class="n">serving_container_shared_memory_size_mb</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">),</span> <span class="c1"># 16 GB</span>
</span><span id="__span-2-21"><a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a> <span class="n">serving_container_deployment_timeout</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
</span><span id="__span-2-22"><a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a> <span class="n">location</span><span class="o">=</span><span class="n">TPU_DEPLOYMENT_REGION</span><span class="p">,</span>
</span><span id="__span-2-23"><a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a><span class="p">)</span>
</span><span id="__span-2-24"><a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>
</span><span id="__span-2-25"><a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a><span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-2-26"><a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a> <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
</span><span id="__span-2-27"><a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-2-28"><a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a> <span class="n">deploy_request_timeout</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span>
</span><span id="__span-2-29"><a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">service_account</span><span class="p">,</span>
</span><span id="__span-2-30"><a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a> <span class="n">min_replica_count</span><span class="o">=</span><span class="n">min_replica_count</span><span class="p">,</span>
</span><span id="__span-2-31"><a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a> <span class="n">max_replica_count</span><span class="o">=</span><span class="n">max_replica_count</span><span class="p">,</span>
</span><span id="__span-2-32"><a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a><span class="p">)</span>
</span></code></pre></div>
<p>Hex-LLM employs prefix caching to optimize performance for prompts exceeding a
certain length (512 tokens by default, configurable using <code>prefill_len_padding</code>).
Cache hits occur in increments of this value, ensuring the cached token count is
always a multiple of <code>prefill_len_padding</code>. The <code>cached_tokens</code> field of
<code>usage.prompt_tokens_details</code> in the chat completion API response indicates how
many of the prompt tokens were a cache hit.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="s2">&quot;usage&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a> <span class="s2">&quot;prompt_tokens&quot;</span><span class="p">:</span> <span class="mi">643</span><span class="p">,</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a> <span class="s2">&quot;total_tokens&quot;</span><span class="p">:</span> <span class="mi">743</span><span class="p">,</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a> <span class="s2">&quot;completion_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a> <span class="s2">&quot;prompt_tokens_details&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a> <span class="s2">&quot;cached_tokens&quot;</span><span class="p">:</span> <span class="mi">512</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a> <span class="p">}</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="p">}</span>
</span></code></pre></div>
<h4 id="4-bit-quantization-support">4-bit quantization support<a class="headerlink" href="#4-bit-quantization-support" title="Permanent link">&para;</a></h4>
<p>Quantization is a technique for reducing the computational and memory costs of
running inference by representing the weights or activations with low-precision
data types like INT8 or INT4 instead of the usual BF16 or FP32.</p>
<p>Hex-LLM supports INT8 weight-only quantization. Extended support includes models
with INT4 weights quantized using AWQ zero-point quantization. Hex-LLM supports
INT4 variants of Mistral, Mixtral and Llama model families.</p>
<p>There is no additional flag required for serving quantized models.</p>
<h2 id="get-started-in-model-garden">Get started in Model Garden<a class="headerlink" href="#get-started-in-model-garden" title="Permanent link">&para;</a></h2>
<p>The Hex-LLM Cloud TPU serving container is integrated into
Model Garden. You can access this serving technology through the
playground, one-click deployment, and Colab Enterprise notebook
examples for a variety of models.</p>
<h3 id="use-playground">Use playground<a class="headerlink" href="#use-playground" title="Permanent link">&para;</a></h3>
<p>Model Garden playground is a pre-deployed Vertex AI
endpoint that is reachable by sending requests in the model card.</p>
<ol>
<li>Enter a prompt and, optionally, include arguments for your request.</li>
<li>Click <strong>SUBMIT</strong> to get the model response quickly.</li>
</ol>
<p><a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335">Try it out with
Gemma</a>!</p>
<h3 id="use-one-click-deployment">Use one-click deployment<a class="headerlink" href="#use-one-click-deployment" title="Permanent link">&para;</a></h3>
<p>You can deploy a custom Vertex AI endpoint with Hex-LLM by using
a model card.</p>
<ol>
<li>Navigate to the <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335">model card page</a>
 and click <strong>Deploy</strong>.</li>
<li>For the model variation that you want to use, select the <a href="https://cloud.google.com/vertex-ai/docs/predictions/use-tpu#deploy_a_model">Cloud TPU
 v5e machine type</a>
 for deployment.</li>
<li>Click <strong>Deploy</strong> at the bottom to begin the deployment process. You receive
 two email notifications; one when the model is uploaded and another when the
 endpoint is ready.</li>
</ol>
<h3 id="use-the-colab-enterprise-notebook">Use the Colab Enterprise notebook<a class="headerlink" href="#use-the-colab-enterprise-notebook" title="Permanent link">&para;</a></h3>
<p>For flexibility and customization, you can use Colab Enterprise
notebook examples to deploy a Vertex AI endpoint with Hex-LLM by
using the Vertex AI SDK for Python.</p>
<ol>
<li>Navigate to the model card page and click <strong>Open notebook</strong>.</li>
<li>Select the Vertex Serving notebook. The notebook is opened in
 Colab Enterprise.</li>
<li>Run through the notebook to deploy a model by using Hex-LLM and send
 prediction requests to the endpoint. The code snippet for the deployment is
 as follows:</li>
</ol>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">hexllm_args</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a> <span class="sa">f</span><span class="s2">&quot;--model=google/gemma-2-9b-it&quot;</span><span class="p">,</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a> <span class="sa">f</span><span class="s2">&quot;--tensor_parallel_size=4&quot;</span><span class="p">,</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a> <span class="sa">f</span><span class="s2">&quot;--hbm_utilization_factor=0.8&quot;</span><span class="p">,</span>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a> <span class="sa">f</span><span class="s2">&quot;--max_running_seqs=512&quot;</span><span class="p">,</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="p">]</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="n">hexllm_envs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a> <span class="s2">&quot;PJRT_DEVICE&quot;</span><span class="p">:</span> <span class="s2">&quot;TPU&quot;</span><span class="p">,</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a> <span class="s2">&quot;MODEL_ID&quot;</span><span class="p">:</span> <span class="s2">&quot;google/gemma-2-9b-it&quot;</span><span class="p">,</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a> <span class="s2">&quot;DEPLOY_SOURCE&quot;</span><span class="p">:</span> <span class="s2">&quot;notebook&quot;</span><span class="p">,</span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="p">}</span>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a> <span class="n">display_name</span><span class="o">=</span><span class="s2">&quot;gemma-2-9b-it&quot;</span><span class="p">,</span>
</span><span id="__span-4-14"><a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a> <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">HEXLLM_DOCKER_URI</span><span class="p">,</span>
</span><span id="__span-4-15"><a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a> <span class="n">serving_container_command</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-4-16"><a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a> <span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;hex_llm.server.api_server&quot;</span>
</span><span id="__span-4-17"><a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a> <span class="p">],</span>
</span><span id="__span-4-18"><a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a> <span class="n">serving_container_args</span><span class="o">=</span><span class="n">hexllm_args</span><span class="p">,</span>
</span><span id="__span-4-19"><a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a> <span class="n">serving_container_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">7080</span><span class="p">],</span>
</span><span id="__span-4-20"><a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a> <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="s2">&quot;/generate&quot;</span><span class="p">,</span>
</span><span id="__span-4-21"><a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a> <span class="n">serving_container_health_route</span><span class="o">=</span><span class="s2">&quot;/ping&quot;</span><span class="p">,</span>
</span><span id="__span-4-22"><a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a> <span class="n">serving_container_environment_variables</span><span class="o">=</span><span class="n">hexllm_envs</span><span class="p">,</span>
</span><span id="__span-4-23"><a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a> <span class="n">serving_container_shared_memory_size_mb</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="__span-4-24"><a id="__codelineno-4-24" name="__codelineno-4-24" href="#__codelineno-4-24"></a> <span class="n">serving_container_deployment_timeout</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
</span><span id="__span-4-25"><a id="__codelineno-4-25" name="__codelineno-4-25" href="#__codelineno-4-25"></a><span class="p">)</span>
</span><span id="__span-4-26"><a id="__codelineno-4-26" name="__codelineno-4-26" href="#__codelineno-4-26"></a>
</span><span id="__span-4-27"><a id="__codelineno-4-27" name="__codelineno-4-27" href="#__codelineno-4-27"></a><span class="n">endpoint</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">display_name</span><span class="o">=</span><span class="s2">&quot;gemma-2-9b-it-endpoint&quot;</span><span class="p">)</span>
</span><span id="__span-4-28"><a id="__codelineno-4-28" name="__codelineno-4-28" href="#__codelineno-4-28"></a><span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-4-29"><a id="__codelineno-4-29" name="__codelineno-4-29" href="#__codelineno-4-29"></a> <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
</span><span id="__span-4-30"><a id="__codelineno-4-30" name="__codelineno-4-30" href="#__codelineno-4-30"></a> <span class="n">machine_type</span><span class="o">=</span><span class="s2">&quot;ct5lp-hightpu-4t&quot;</span><span class="p">,</span>
</span><span id="__span-4-31"><a id="__codelineno-4-31" name="__codelineno-4-31" href="#__codelineno-4-31"></a> <span class="n">deploy_request_timeout</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span>
</span><span id="__span-4-32"><a id="__codelineno-4-32" name="__codelineno-4-32" href="#__codelineno-4-32"></a> <span class="n">service_account</span><span class="o">=</span><span class="s2">&quot;&lt;your-service-account&gt;&quot;</span><span class="p">,</span>
</span><span id="__span-4-33"><a id="__codelineno-4-33" name="__codelineno-4-33" href="#__codelineno-4-33"></a> <span class="n">min_replica_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-4-34"><a id="__codelineno-4-34" name="__codelineno-4-34" href="#__codelineno-4-34"></a> <span class="n">max_replica_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-4-35"><a id="__codelineno-4-35" name="__codelineno-4-35" href="#__codelineno-4-35"></a><span class="p">)</span>
</span></code></pre></div>
<p>Example Colab Enterprise notebooks include:</p>
<ul>
<li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma2_deployment_on_vertex.ipynb">Gemma 2 deployment</a></li>
<li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_codegemma_deployment_on_vertex.ipynb">CodeGemma deployment</a></li>
<li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_2_deployment.ipynb">Llama 3.2 deployment</a></li>
<li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_1_deployment.ipynb">Llama 3.1 deployment</a></li>
<li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_phi3_deployment.ipynb">Phi-3 deployment</a></li>
<li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_qwen2_deployment.ipynb">Qwen2 deployment</a></li>
</ul>
<h3 id="configure-server-arguments-and-environment-variables">Configure server arguments and environment variables<a class="headerlink" href="#configure-server-arguments-and-environment-variables" title="Permanent link">&para;</a></h3>
<p>You can set the following arguments to launch the Hex-LLM server. You can tailor
the arguments to best fit your intended use case and requirements. Note that the
arguments are predefined for one-click deployment for enabling the easiest
deployment experience. To customize the arguments, you can build off of the
notebook examples for reference and set the arguments accordingly.</p>
<p><em>Model</em></p>
<ul>
<li><code>--model</code>: The model to load. You can specify a Hugging Face model ID, a
 Cloud Storage bucket path (<code>gs://my-bucket/my-model</code>), or a local path.
 The model artifacts are expected to follow the Hugging Face format and use
 <a href="https://huggingface.co/docs/safetensors/en/index">safetensors</a> files for
 the model weights. <a href="https://huggingface.co/docs/bitsandbytes/main/en/index">BitsAndBytes</a>
 int8 and <a href="https://huggingface.co/docs/transformers/main/en/quantization/awq">AWQ</a>
 quantized model artifacts are supported for Llama, Gemma 2 and
 Mistral/Mixtral.</li>
<li><code>--tokenizer</code>: The <a href="https://huggingface.co/docs/transformers/en/main_classes/tokenizer">tokenizer</a>
 to load. This can be a Hugging Face model ID, a <a href="https://cloud.google.com/storage">Cloud Storage</a>
 bucket path (<code>gs://my-bucket/my-model</code>), or a local path. If this argument
 is not set, it defaults to the value for <code>--model</code>.</li>
<li><code>--tokenizer_mode</code>: The tokenizer mode. Possible choices are
 <code>["auto", "slow"]</code>. The default value is <code>"auto"</code>. If this is set to
 <code>"auto"</code>, the fast tokenizer is used if available. The slow tokenizers are
 written in Python and provided in the Transformers library, while the fast
 tokenizers offering performance improvement are written in Rust and provided
 in the Tokenizers library. For more information, see the <a href="https://huggingface.co/learn/nlp-course/chapter6/3">Hugging Face documentation</a>.</li>
<li><code>--trust_remote_code</code>: Whether to allow remote code files defined in the
 Hugging Face model repositories. The default value is <code>False</code>.</li>
<li><code>--load_format</code>: Format of model checkpoints to load. Possible choices are
 <code>["auto", "dummy"]</code>. The default value is <code>"auto"</code>. If this is set to
 <code>"auto"</code>, the model weights are loaded in safetensors format. If this is set
 to <code>"dummy"</code>, the model weights are randomly initialized. Setting this to
 <code>"dummy"</code> is useful for experimentation.</li>
<li><code>--max_model_len</code>: The maximum context length (input length plus the output
 length) to serve for the model. The default value is read from the model
 configuration file in Hugging Face format: <code>config.json</code>. A larger maximum
 context length requires more TPU memory.</li>
<li><code>--sliding_window</code>: If set, this argument overrides the model's window size
 for <a href="https://arxiv.org/abs/2004.05150">sliding window attention</a>. Setting
 this argument to a larger value makes the attention mechanism include more
 tokens and approaches the effect of standard self attention. This argument
 is meant for experimental usage only. In general use cases, we recommend
 using the model's original window size.</li>
<li><code>--seed</code>: The seed for initializing all random number generators. Changing
 this argument might affect the generated output for the same prompt through
 changing the tokens that are sampled as next tokens. The default value is
 <code>0</code>.</li>
</ul>
<p><em>Inference engine</em></p>
<ul>
<li><code>--num_hosts</code>: The number of hosts to run. The default value is <code>1</code>. For
 more details, refer to the documentation on <a href="https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config">TPU v5e configuration</a>.</li>
<li><code>--disagg_topo</code>: Defines the number of prefill workers and decode workers
 with the experimental feature disaggregated serving. The default value is
 <code>None</code>. The argument follows the format: <code>"number_of_prefill_workers,number_of_decode_workers"</code>.</li>
<li><code>--data_parallel_size</code>: The number of data parallel replicas. The default
 value is <code>1</code>. Setting this to <code>N</code> from <code>1</code> approximately improves the
 throughput by <code>N</code>, while maintaining the same latency.</li>
<li><code>--tensor_parallel_size</code>: The number of tensor parallel replicas. The
 default value is <code>1</code>. Increasing the number of tensor parallel replicas
 generally improves latency, because it speeds up matrix multiplication by
 reducing the matrix size.</li>
<li><code>--worker_distributed_method</code>: The distributed method to launch the worker.
 Use <code>mp</code> for the <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a>
 module or <code>ray</code> for the <a href="https://docs.ray.io/">Ray</a> library. The default
 value is <code>mp</code>.</li>
<li><code>--enable_jit</code>: Whether to enable <a href="https://jax.readthedocs.io/en/latest/jit-compilation.html">JIT (Just-in-Time Compilation)</a>
 mode. The default value is <code>True</code>. Setting <code>--no-enable_jit</code> disables it.
 Enabling JIT mode improves inference performance at the cost of requiring
 additional time spent on initial compilation. In general, the inference
 performance benefits overweigh the overhead.</li>
<li><code>--warmup</code>: Whether to warm up the server with sample requests during
 initialization. The default value is <code>True</code>. Setting <code>--no-warmup</code> disables
 it. Warmup is recommended, because initial requests trigger heavier
 compilation and therefore will be slower.</li>
<li><code>--max_prefill_seqs</code>: The maximum number of sequences that can be scheduled
 for prefilling per iteration. The default value is <code>1</code>. The larger this
 value is, the higher throughput the server can achieve, but with potential
 adverse effects on latency.</li>
<li><code>--prefill_seqs_padding</code>: The server pads the prefill batch size to a
 multiple of this value. The default value is <code>8</code>. Increasing this value
 reduces model recompilation times, but increases wasted computation and
 inference overhead. The optimal setting depends on the request traffic.</li>
<li><code>--prefill_len_padding</code>: The server pads the sequence length to a multiple
 of this value. The default value is <code>512</code>. Increasing this value reduces
 model recompilation times, but increases wasted computation and inference
 overhead. The optimal setting depends on the data distribution of the
 requests.</li>
<li><code>--max_decode_seqs</code>/<code>--max_running_seqs</code>: The maximum number of sequences
 that can be scheduled for decoding per iteration. The default value is <code>256</code>.
 The larger this value is, the higher throughput the server can achieve, but
 with potential adverse effects on latency.</li>
<li><code>--decode_seqs_padding</code>: The server pads the decode batch size to a multiple
 of this value. The default value is <code>8</code>. Increasing this value reduces model
 recompilation times, but increases wasted computation and inference overhead.
 The optimal setting depends on the request traffic.</li>
<li><code>--decode_blocks_padding</code>: The server pads the number of memory blocks used
 for a sequence's Key-Value cache (KV cache) to a multiple of this value
 during decoding. The default value is <code>128</code>. Increasing this value reduces
 model recompilation times, but increases wasted computation and inference
 overhead. The optimal setting depends on the data distribution of the
 requests.</li>
<li><code>--enable_prefix_cache_hbm</code>: Whether to enable <a href="#prefix-caching">prefix caching</a>
 in HBM. The default value is <code>False</code>. Setting this argument can improve
 performance by reusing the computations of shared prefixes of prior requests.</li>
</ul>
<p><em>Memory management</em></p>
<ul>
<li><code>--hbm_utilization_factor</code>: The percentage of free <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Cloud TPU High Bandwidth Memory (HBM)</a>
 that can be allocated for KV cache after model weights are loaded. The
 default value is <code>0.9</code>. Setting this argument to a higher value increases
 the KV cache size and can improve throughput, but it increases the risk of
 running out of Cloud TPU HBM during initialization and at runtime.</li>
<li><code>--num_blocks</code>: Number of device blocks to allocate for KV cache. If this
 argument is set, the server ignores <code>--hbm_utilization_factor</code>. If this
 argument is not set, the server profiles HBM usage and computes the number
 of device blocks to allocate based on <code>--hbm_utilization_factor</code>. Setting
 this argument to a higher value increases the KV cache size and can improve
 throughput, but it increases the risk of running out of Cloud TPU HBM during
 initialization and at runtime.</li>
<li><code>--block_size</code>: Number of tokens stored in a block. Possible choices are
 <code>[8, 16, 32, 2048, 8192]</code>. The default value is <code>32</code>. Setting this argument
 to a larger value reduces overhead in block management, at the cost of more
 memory waste. The exact performance impact needs to be determined
 empirically.</li>
</ul>
<p><em>Dynamic LoRA</em></p>
<ul>
<li><code>--enable_lora</code>: Whether to enable dynamic <a href="https://arxiv.org/abs/2106.09685">LoRA adapters</a>
 loading from Cloud Storage. The default value is <code>False</code>. This is
 supported for the Llama model family.</li>
<li><code>--max_lora_rank</code>: The maximum LoRA rank supported for LoRA adapters defined
 in requests. The default value is <code>16</code>. Setting this argument to a higher
 value allows for greater flexibility in the LoRA adapters that can be used
 with the server, but increases the amount of Cloud TPU HBM allocated for
 LoRA weights and decreases throughput.</li>
<li><code>--enable_lora_cache</code>: Whether to enable caching of dynamic LoRA adapters.
 The default value is <code>True</code>. Setting <code>--no-enable_lora_cache</code> disables it.
 Caching improves performance because it removes the need to re-download
 previously used LoRA adapter files.</li>
<li><code>--max_num_mem_cached_lora</code>: The maximum number of LoRA adapters stored in
 TPU memory cache.The default value is <code>16</code>. Setting this argument to a
 larger value improves the chance of a cache hit, but it increases the amount
 of Cloud TPU HBM usage.</li>
</ul>
<p>You can also configure the server using the following environment variables:</p>
<ul>
<li><code>HEX_LLM_LOG_LEVEL</code>: Controls the amount of logging information generated.
 The default value is <code>INFO</code>. Set this to one of the standard Python logging
 levels defined in the <a href="https://docs.python.org/3/library/logging.html#logging-levels">logging module</a>.</li>
<li><code>HEX_LLM_VERBOSE_LOG</code>: Whether to enable detailed logging output. Allowed
 values are <code>true</code> or <code>false</code>. Default value is <code>false</code>.</li>
</ul>
<h4 id="tune-server-arguments">Tune server arguments<a class="headerlink" href="#tune-server-arguments" title="Permanent link">&para;</a></h4>
<p>The server arguments are interrelated and have a collective effect on the
serving performance. For example, a larger setting of <code>--max_model_len=4096</code>
leads to higher TPU memory usage, and therefore requires larger memory
allocation and less batching. In addition, some arguments are determined by the
use case, while others can be tuned. Here is a workflow for configuring the
Hex-LLM server.</p>
<ol>
<li>Determine the model family and model variant of interest. For example, Llama
 3.1 8B Instruct.</li>
<li>Estimate the lower bound of TPU memory needed based on the model size and
 precision: <code>model_size * (num_bits / 8)</code>. For an 8B model and bfloat16
 precision, the lower bound of TPU memory needed would be
 <code>8 * (16 / 8) = 16 GB</code>.</li>
<li>Estimate the number of TPU v5e chips needed, where each v5e chip offers 16GB:
 <code>tpu_memory / 16</code>. For an 8B model and bfloat16 precision, you need more
 than 1 chip. Among the <a href="https://cloud.google.com/vertex-ai/docs/predictions/use-tpu#deploy_a_model">1-chip, 4-chip and 8-chip configurations</a>,
 the smallest configuration that offers more than 1 chip is the 4-chip
 configuration: <code>ct5lp-hightpu-4t</code>. You can subsequently set
 <code>--tensor_parallel_size=4</code>.</li>
<li>Determine the maximum context length (input length + output length) for the
 intended use case. For example, 4096. You can subsequently set
 <code>--max_model_len=4096</code>.</li>
<li>Tune the amount of free TPU memory allocated for KV cache to the maximum
 value achievable given the model, hardware and server configurations
 (<code>--hbm_utilization_factor</code>). Start with <code>0.95</code>. Deploy the Hex-LLM server
 and test the server with long prompts and high concurrency. If the server
 runs out-of-memory, reduce the utilization factor accordingly.</li>
</ol>
<p>A sample set of arguments for deploying Llama 3.1 8B Instruct is:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">hex_llm</span><span class="o">.</span><span class="n">server</span><span class="o">.</span><span class="n">api_server</span> \
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a> <span class="o">--</span><span class="n">model</span><span class="o">=</span><span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a> <span class="o">--</span><span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">4</span> \
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a> <span class="o">--</span><span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a> <span class="o">--</span><span class="n">hbm_utilization_factor</span><span class="o">=</span><span class="mf">0.95</span>
</span></code></pre></div>
<p>A sample set of arguments for deploying Llama 3.1 70B Instruct AWQ on
<code>ct5lp-hightpu-4t</code> is:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">hex_llm</span><span class="o">.</span><span class="n">server</span><span class="o">.</span><span class="n">api_server</span> \
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a> <span class="o">--</span><span class="n">model</span><span class="o">=</span><span class="n">hugging</span><span class="o">-</span><span class="n">quants</span><span class="o">/</span><span class="n">Meta</span><span class="o">-</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">70</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span><span class="o">-</span><span class="n">AWQ</span><span class="o">-</span><span class="n">INT4</span> \
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a> <span class="o">--</span><span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">4</span> \
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a> <span class="o">--</span><span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a> <span class="o">--</span><span class="n">hbm_utilization_factor</span><span class="o">=</span><span class="mf">0.45</span>
</span></code></pre></div>
<h3 id="request-cloud-tpu-quota">Request Cloud TPU quota<a class="headerlink" href="#request-cloud-tpu-quota" title="Permanent link">&para;</a></h3>
<p>In Model Garden, your default quota is 4 Cloud TPU v5e
chips in the <code>us-west1</code> region. This quotas applies to one-click deployments and
Colab Enterprise notebook deployments. To request additional quotas,
see <a href="https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota">Request a higher
quota</a>.</p>







  
  






                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.action.edit", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.progress", "navigation.path", "navigation.top", "navigation.tracking", "toc.follow", "navigation.tabs", "navigation.sections", "navigation.tracking", "navigation.top", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../js/chat-widget/gemini-client.js"></script>
      
        <script src="../../js/chat-widget/init.js"></script>
      
        <script src="../../js/chat-widget/setup-key.js"></script>
      
        <script src="../../js/chat-widget-helper.js"></script>
      
    
  </body>
</html>