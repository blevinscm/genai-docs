
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://blevinscm.github.com/genai-docs/open-models/vllm/use-vllm/">
      
      
      
      
      <link rel="icon" href="../../../assets/google-cloud-vertex-ai.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Vllm Serving For Text Only And Multimodal Language Models On Cloud Gpusstay Organized With Collectio - Vertex Generative AI</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Google Sans";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../../js/chat-widget/chat-widget.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vllm-serving-for-text-only-and-multimodal-language-models-on-cloud-gpus" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Vertex Generative AI" class="md-header__button md-logo" aria-label="Vertex Generative AI" data-md-component="logo">
      
  <img src="../../../assets/google-cloud-vertex-ai.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Vertex Generative AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Vllm Serving For Text Only And Multimodal Language Models On Cloud Gpusstay Organized With Collectio
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/blevinscm/genai-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Generative-AI-on-Vertex-AI-Cookbook/" class="md-tabs__link">
          
  
  
    
  
  Learn

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../agent-engine/Set-up-the-environment/" class="md-tabs__link">
          
  
  
    
  
  Build

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../models/Introduction-to-tuning/" class="md-tabs__link">
          
  
  
    
  
  Tune

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../deploy/Deploy-generative-AI-models/" class="md-tabs__link">
          
  
  
    
  
  Deploy

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../agent-engine/manage/Manage-deployed-agents/" class="md-tabs__link">
          
  
  
    
  
  Manage

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../models/evaluation-overview/" class="md-tabs__link">
          
  
  
    
  
  Optimize

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Vertex Generative AI" class="md-nav__button md-logo" aria-label="Vertex Generative AI" data-md-component="logo">
      
  <img src="../../../assets/google-cloud-vertex-ai.png" alt="logo">

    </a>
    Vertex Generative AI
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/blevinscm/genai-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Learn
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Learn
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Generative-AI-on-Vertex-AI-Cookbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vertex AI Generative AI Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Generative-AI-glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Generative-AI-on-Vertex-AI-release-notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Release Notes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supported-models_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supported Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../model-reference/gemini/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gemini Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../image/Imagen-on-Vertex-AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Imagen Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../code/code-models-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Code Models Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../intelligent-code-commenter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Intelligent Code Commenter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Use-Gemma-open-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Open Models (Gemma, Llama)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../partner-models/use-partner-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Partner Models (Claude, AI21)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learn/prompts/Introduction-to-prompting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction to Prompting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learn/prompts/Overview-of-prompting-strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompting Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../multimodal/Design-multimodal-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Concepts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../embeddings/Choose-an-embeddings-task-type/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../multimodal/Introduction-to-function-calling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Function Calling Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../grounding/Ground-responses-using-RAG/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Grounding Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-builder/Vertex-AI-Agent-Builder-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Builder Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/Vertex-AI-Agent-Engine-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Engine Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rag-engine/Vertex-AI-RAG-Engine-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Engine Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../context-cache/Context-caching-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Context Caching Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../extensions/Extensions-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Extensions Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../example-store/Example-Store-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Example Store Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Generative-AI-and-data-governance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Governance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Security-controls-for-Generative-AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Security & Safety
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../deprecations/Model-versions-and-lifecycle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deprecations & Lifecycle
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting-help_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Help
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Build
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Build
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/Set-up-the-environment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Set up Environment (Agent Engine)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/develop/Develop-a-LangChain-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Develop LangChain Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/develop/Develop-an-Agent-Development-Kit-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Develop ADK Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/use/Use-a-LangChain-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use LangChain Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/use/Use-a-LangGraph-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use LangGraph Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chat/Design-chat-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Design Chat Prompts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../multimodal/Text-generationbookmark_borderbookmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Text with Gemini
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../image/Generate-images-using-text-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Images with Imagen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Generate-images-with-Gemini/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Images with Gemini
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../video/generate-videos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generate Video with Veo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../embeddings/Get-batch-text-embeddings-predictions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Get Text Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../embeddings/Get-multimodal-embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Get Multimodal Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../model-reference/Function-calling-reference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Function Calling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../grounding/Use-Google-Search-suggestions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Grounding (Google Search)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../grounding/Grounding-with-your-data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Grounding (Your Data)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rag-engine/Use-data-ingestion-with-Vertex-AI-RAG-Engine/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Build with RAG Engine (Data Ingestion)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rag-engine/use-vertexai-vector-search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Build with RAG Engine (Vector Search)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Use-a-Weaviate-database-with-Vertex-AI-RAG-Engine/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Build with RAG Engine (Weaviate)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../context-cache/Create-a-context-cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create Context Cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../extensions/Create-and-run-extensions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create Extensions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../example-store/Retrieve-examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Example Store
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../multimodal/audio-understanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Audio Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../multimodal/Document-understanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Document Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../multimodal/Video-understanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal Video Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../prompt-gallery/samples/summarize_summarize_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Gallery Samples
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tune
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Tune
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/Introduction-to-tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction to Model Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/About-supervised-fine-tuning-for-Gemini-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Gemini Models (Supervised)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/gemini-supervised-tuning-prepare/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prepare Data for Gemini SFT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/tune_gemini/text_tune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Gemini for Text
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../image/Style-customization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Imagen Models (Style/Subject)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/Tune-function-calling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tune Function Calling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../model-garden/LoRA-and-QLoRA-recommendations-for-LLMs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA & QLoRA Recommendations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Fine-tune-RAG-transformations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tune RAG Transformations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deploy
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Deploy
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../deploy/Deploy-generative-AI-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deploy Generative AI Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/deploy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deploy Agent (Agent Engine)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/Deployments-and-endpoints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deployments and Endpoints
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../model-garden/Use-models-in-Model-Garden/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Models from Model Garden
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Purchase-Provisioned-Throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Purchase Provisioned Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Use-Provisioned-Throughput/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Provisioned Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../migrate/Migrate-to-the-Gemini-API-from-Azure-OpenAI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Migrate from Azure OpenAI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../migrate/openai/Using-OpenAI-libraries-with-Vertex-AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use OpenAI Libraries with Vertex AI
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Manage
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Manage
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/manage/Manage-deployed-agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Deployed Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/manage/logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/manage/monitoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/sessions/Manage-sessions-using-direct-API-calls/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Agent Sessions (API)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/sessions/Manage-sessions-with-Agent-Development-Kit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Agent Sessions (ADK)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../context-cache/Get-information-about-a-context-cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage Context Cache (Get/Delete)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Manage-your-RAG-knowledge-base-corpus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Manage RAG Corpus
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learn/Model-monitoring-metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Monitoring Metrics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../quotas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quotas & Limits
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/troubleshooting/Troubleshoot-deploying-an-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshoot Agent Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agent-engine/troubleshooting/Troubleshoot-developing-an-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshoot Agent Development
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Optimize
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Optimize
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/evaluation-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Evaluation Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/Gen-AI-evaluation-service-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gen AI Evaluation Service
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/Run-AutoSxS-pipeline-to-perform-pairwise-model-based-evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Run AutoSxS (Pairwise Evaluation)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../models/Evaluate-Gen-AI-agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluate Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../learn/prompts/Optimize-promptsbookmark_borderbookmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimize Prompts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Retrieval-and-ranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Retrieval and Ranking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../model-reference/Vertex-AI-Model-Optimizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vertex AI Model Optimizer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../provisioned-throughput/Calculate-Provisioned-Throughput-requirements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Calculate Provisioned Throughput
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../model-reference/count-tokens/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Count Tokens
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../context-cache/Context-caching-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Context Caching Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vllm-key-features" class="md-nav__link">
    <span class="md-ellipsis">
      vLLM Key Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="vLLM Key Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#google-vertex-ai-vllm-customizations-enhance-performance-and-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Google Vertex AI vLLM Customizations: Enhance performance and integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-benefits-of-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Additional benefits of vLLM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supported-models" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#get-started-in-model-garden" class="md-nav__link">
    <span class="md-ellipsis">
      Get started in Model Garden
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Get started in Model Garden">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-the-colab-enterprise-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      Use the Colab Enterprise notebook
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#setup-and-requirements" class="md-nav__link">
    <span class="md-ellipsis">
      Setup and requirements
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Setup and requirements">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-billing" class="md-nav__link">
    <span class="md-ellipsis">
      1. Billing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-gpu-availability-and-quotas" class="md-nav__link">
    <span class="md-ellipsis">
      2. GPU availability and quotas
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-set-up-a-google-cloud-project" class="md-nav__link">
    <span class="md-ellipsis">
      3. Set up a Google Cloud Project
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-hugging-face-with-meta-llama-31-32-and-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Using Hugging Face with Meta Llama 3.1, 3.2, and vLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Hugging Face with Meta Llama 3.1, 3.2, and vLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overview-of-meta-llama-31-and-32-collections" class="md-nav__link">
    <span class="md-ellipsis">
      Overview of Meta Llama 3.1 and 3.2 Collections
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hugging-face-user-access-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      Hugging Face user access tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deploying-text-only-llama-31-models-with-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Deploying text-only Llama 3.1 Models with vLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deploying text-only Llama 3.1 Models with vLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-choose-a-model-to-deploy" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Choose a model to deploy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-check-deployment-hardware-and-quota" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Check deployment hardware and quota
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-inspect-the-model-using-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Inspect the model using vLLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-execute-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Execute deployment
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#making-predictions-with-llama-31-on-vertex-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Making predictions with Llama 3.1 on Vertex AI
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Making predictions with Llama 3.1 on Vertex AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-define-your-prompt-and-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Define your prompt and parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-send-the-prediction-request" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Send the prediction request
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-output" class="md-nav__link">
    <span class="md-ellipsis">
      Example output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-notes" class="md-nav__link">
    <span class="md-ellipsis">
      Additional notes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deploying-multimodal-llama-32-models-with-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Deploying multimodal Llama 3.2 models with vLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deploying multimodal Llama 3.2 models with vLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-choose-a-model-to-deploy_1" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Choose a model to deploy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-configure-hardware-and-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Configure hardware and resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-deploy-the-model-using-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Deploy the model using vLLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-execute-deployment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Execute deployment
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-with-vllm-on-vertex-ai-using-default-prediction-route" class="md-nav__link">
    <span class="md-ellipsis">
      Inference with vLLM on Vertex AI using default prediction route
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Inference with vLLM on Vertex AI using default prediction route">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-define-your-prompt-and-parameters_1" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Define your prompt and parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-configure-prediction-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Configure prediction parameters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-prepare-the-prediction-request" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Prepare the prediction request
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-make-the-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Make the prediction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-with-vllm-on-vertex-ai-using-openai-chat-completion" class="md-nav__link">
    <span class="md-ellipsis">
      Inference with vLLM on Vertex AI using OpenAI Chat Completion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Inference with vLLM on Vertex AI using OpenAI Chat Completion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-execute-deployment-of-llama-32-vision-instruct-model" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Execute deployment of Llama 3.2 Vision Instruct model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-configure-endpoint-resource" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Configure endpoint resource
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-install-openai-sdk-and-authentication-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Install OpenAI SDK and authentication libraries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-define-input-parameters-for-chat-completion" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Define input parameters for chat completion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-set-up-authentication-and-base-url" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5: Set up authentication and base URL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-send-chat-completion-request" class="md-nav__link">
    <span class="md-ellipsis">
      Step 6: Send Chat Completion request
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optional-step-7-reconnect-to-an-existing-endpoint" class="md-nav__link">
    <span class="md-ellipsis">
      (Optional ) Step 7: Reconnect to an existing endpoint
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      Cleanup
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cleanup">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-delete-endpoints-and-models" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Delete Endpoints and Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-optional-delete-cloud-storage-bucket" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: (Optional) Delete Cloud Storage Bucket
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#debugging-common-issues" class="md-nav__link">
    <span class="md-ellipsis">
      Debugging common issues
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Debugging common issues">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-the-logs" class="md-nav__link">
    <span class="md-ellipsis">
      Check the logs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-issue-1-cuda-out-of-memory-oom-during-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      Common Issue 1: CUDA Out of Memory (OOM) during deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-issue-2-hugging-face-token-needed" class="md-nav__link">
    <span class="md-ellipsis">
      Common Issue 2: Hugging Face token needed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-issue-3-chat-template-needed" class="md-nav__link">
    <span class="md-ellipsis">
      Common Issue 3: Chat template needed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-issue-4-model-max-seq-len" class="md-nav__link">
    <span class="md-ellipsis">
      Common Issue 4: Model Max Seq Len
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/blevinscm/genai-docs/edit/main/docs/open-models/vllm/use-vllm.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="vllm-serving-for-text-only-and-multimodal-language-models-on-cloud-gpus">vLLM serving for text-only and multimodal language models on Cloud GPUs<a class="headerlink" href="#vllm-serving-for-text-only-and-multimodal-language-models-on-cloud-gpus" title="Permanent link">&para;</a></h1>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<p>This tutorial walks you through the process of deploying and serving Llama 3.1 and 3.2 models using <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a> in Vertex AI. It is designed to be used in conjunction with two separate notebooks: <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_vllm_text_only_tutorial.ipynb">Serve Llama 3.1 with vLLM</a> for deploying text-only Llama 3.1 models, and <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_vllm_multimodal_tutorial.ipynb">Serve Multimodal Llama 3.2 with vLLM</a> for deploying multimodel Llama 3.2 models that handle both text and image inputs. The steps outlined on this page show you how to efficiently handle model inference on GPUs and customize models for diverse applications, equipping you with the tools to integrate advanced language models into your projects.</p>
<p>By the end of this guide, you will understand how to:</p>
<ul>
<li>Download prebuilt Llama models from <a href="https://huggingface.co">Hugging Face</a> with vLLM container.</li>
<li>Use vLLM to deploy these models on GPU instances within Google Cloud Vertex AI Model Garden.</li>
<li>Serve models efficiently to handle inference requests at scale.</li>
<li>Run inference on text-only requests and text + image requests.</li>
<li>Cleanup.</li>
<li>Debug deployment.</li>
</ul>
<h2 id="vllm-key-features">vLLM Key Features<a class="headerlink" href="#vllm-key-features" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th><strong>Feature</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PagedAttention</strong></td>
<td>An optimized attention mechanism that efficiently manages memory during inference. Supports high-throughput text generation by dynamically allocating memory resources, enabling scalability for multiple concurrent requests.</td>
</tr>
<tr>
<td><strong>Continuous batching</strong></td>
<td>Consolidates multiple input requests into a single batch for parallel processing, maximizing GPU utilization and throughput.</td>
</tr>
<tr>
<td><strong>Token streaming</strong></td>
<td>Enables real-time token-by-token output during text generation. Ideal for applications that require low latency, such as chatbots or interactive AI systems.</td>
</tr>
<tr>
<td><strong>Model compatibility</strong></td>
<td>Supports a wide range of pre-trained models across popular frameworks like Hugging Face Transformers. Makes it easier to integrate and experiment with different LLMs.</td>
</tr>
<tr>
<td><strong>Multi-GPU &amp; multi-host</strong></td>
<td>Enables efficient model serving by distributing the workload across multiple GPUs within a single machine and across multiple machines in a cluster, significantly increasing throughput and scalability.</td>
</tr>
<tr>
<td><strong>Efficient deployment</strong></td>
<td>Offers seamless integration with APIs, such as OpenAI chat completions, making deployment straightforward for production use cases.</td>
</tr>
<tr>
<td><strong>Seamless integration with Hugging Face models</strong></td>
<td>vLLM is compatible with Hugging Face model artifacts format and supports loading from HF, making it straightforward to deploy Llama models alongside other popular models like Gemma, Phi, and Qwen in an optimized setting.</td>
</tr>
<tr>
<td><strong>Community-driven open-source project</strong></td>
<td>vLLM is open-source and encourages community contributions, promoting continuous improvement in LLM serving efficiency.</td>
</tr>
</tbody>
</table>
<p><strong>Table 1: Summary of vLLM features</strong></p>
<h3 id="google-vertex-ai-vllm-customizations-enhance-performance-and-integration">Google Vertex AI vLLM Customizations: Enhance performance and integration<a class="headerlink" href="#google-vertex-ai-vllm-customizations-enhance-performance-and-integration" title="Permanent link">&para;</a></h3>
<p>The vLLM implementation within Google Vertex AI Model Garden is not a direct integration of the open-source library. Vertex AI maintains a customized and optimized version of vLLM that is specifically tailored to enhance performance, reliability, and seamless integration within the Google Cloud.</p>
<ul>
<li><strong>Performance optimizations:</strong></li>
<li><strong>Parallel downloading from Cloud Storage:</strong> Significantly accelerates model loading and deployment times by enabling parallel data retrieval from Cloud Storage, reducing latency and improving startup speed.</li>
<li><strong>Feature enhancements:</strong></li>
<li><strong>Dynamic LoRA with enhanced caching and Cloud Storage support:</strong> Extends dynamic LoRA capabilities with local disk caching mechanisms and robust error handling, alongside support for loading LoRA weights directly from Cloud Storage paths and signed URLs. This simplifies management and deployment of customized models.</li>
<li><strong>Llama 3.1/3.2 function calling parsing:</strong> Implements specialized parsing for Llama 3.1/3.2 function calling, improving the robustness in parsing.</li>
<li><strong>Host memory prefix caching:</strong> The external vLLM only supports GPU memory prefix caching.</li>
<li><strong>Speculative decoding:</strong> This is an existing vLLM feature, but Vertex AI ran experiments to find high-performing model setups.</li>
</ul>
<p>These Vertex AI-specific customizations, while often transparent to the end-user, enable you to maximize the performance and efficiency of your Llama 3.1 deployments on Vertex AI Model Garden.</p>
<ul>
<li><strong>Vertex AI ecosystem integration:</strong></li>
<li><strong>Vertex AI prediction input/output format support:</strong> Ensures seamless compatibility with Vertex AI prediction input and output formats, simplifying data handling and integration with other Vertex AI services.</li>
<li><strong>Vertex Environment variable awareness:</strong> Respects and leverages Vertex AI environment variables (<code>AIP_*</code>) for configuration and resource management, streamlining deployment and ensuring consistent behavior within the Vertex AI environment.</li>
<li><strong>Enhanced error handling and robustness:</strong> Implements comprehensive error handling, input/output validation, and server termination mechanisms to ensure stability, reliability, and seamless operation within the managed Vertex AI environment.</li>
<li><strong>Nginx server for capability:</strong> Integrates an Nginx server on top of the vLLM server, facilitating the deployment of multiple replicas and enhancing scalability and high availability of the serving infrastructure.</li>
</ul>
<h3 id="additional-benefits-of-vllm">Additional benefits of vLLM<a class="headerlink" href="#additional-benefits-of-vllm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Benchmark performance</strong>: vLLM offers competitive performance when compared to other serving systems like Hugging Face text-generation-inference and NVIDIA's FasterTransformer in terms of throughput and latency.</li>
<li><strong>Ease of use</strong>: The library provides a straightforward API for integration with existing workflows, allowing you to deploy both Llama 3.1 and 3.2 models with minimal setup.</li>
<li><strong>Advanced features</strong>: vLLM supports streaming outputs (generating responses token-by-token) and efficiently handles variable-length prompts, enhancing interactivity and responsiveness in applications.</li>
</ul>
<p>For an overview of the vLLM system, see the <a href="https://dl.acm.org/doi/pdf/10.1145/3600006.3613165">paper</a>.</p>
<h2 id="supported-models">Supported Models<a class="headerlink" href="#supported-models" title="Permanent link">&para;</a></h2>
<p>vLLM provides support for a broad selection of state-of-the-art models, allowing you to choose a model that best fits your needs. The following table offers a selection of these models. However, to access a comprehensive list of supported models, including those for both text-only and multimodal inference, you can consult the official vLLM <a href="https://docs.vllm.ai/en/latest/models/supported_models.html">website</a>.</p>
<table>
<thead>
<tr>
<th><strong>Category</strong></th>
<th><strong>Models</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Meta AI</strong></td>
<td>Llama 3.3, Llama 3.2, Llama 3.1, Llama 3, Llama 2, Code Llama</td>
</tr>
<tr>
<td><strong>Mistral AI</strong></td>
<td>Mistral 7B, Mixtral 8x7B, Mixtral 8x22B, and their variants (Instruct, Chat), Mistral-tiny, Mistral-small, Mistral-medium</td>
</tr>
<tr>
<td><strong>DeepSeek AI</strong></td>
<td>DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, DeepSeek-R1-Distill-Qwen-14B, DeepSeek-R1-Distill-Qwen-32B, DeepSeek-R1-Distill-Llama-70B, Deepseek-vl2-tiny, Deepseek-vl2-small, Deepseek-vl2</td>
</tr>
<tr>
<td><strong>MosaicML</strong></td>
<td>MPT (7B, 30B) and variants (Instruct, Chat), MPT-7B-StoryWriter-65k</td>
</tr>
<tr>
<td><strong>OpenAI</strong></td>
<td>GPT-2, GPT-3, GPT-4, GPT-NeoX</td>
</tr>
<tr>
<td><strong>Together AI</strong></td>
<td>RedPajama, Pythia</td>
</tr>
<tr>
<td><strong>Stability AI</strong></td>
<td>StableLM (3B, 7B), StableLM-Alpha-3B, StableLM-Base-Alpha-7B, StableLM-Instruct-Alpha-7B</td>
</tr>
<tr>
<td><strong>TII (Technology Innovation Institute)</strong></td>
<td>Falcon 7B, Falcon 40B and variants (Instruct, Chat), Falcon-RW-1B, Falcon-RW-7B</td>
</tr>
<tr>
<td><strong>BigScience</strong></td>
<td>BLOOM, BLOOMZ</td>
</tr>
<tr>
<td><strong>Google</strong></td>
<td>FLAN-T5, UL2, Gemma (2B, 7B), PaLM 2,</td>
</tr>
<tr>
<td><strong>Salesforce</strong></td>
<td>CodeT5, CodeT5+</td>
</tr>
<tr>
<td><strong>LightOn</strong></td>
<td>Persimmon-8B-base, Persimmon-8B-chat</td>
</tr>
<tr>
<td><strong>EleutherAI</strong></td>
<td>GPT-Neo, Pythia</td>
</tr>
<tr>
<td><strong>AI21 Labs</strong></td>
<td>Jamba</td>
</tr>
<tr>
<td><strong>Cerebras</strong></td>
<td>Cerebras-GPT</td>
</tr>
<tr>
<td><strong>Intel</strong></td>
<td>Intel-NeuralChat-7B</td>
</tr>
<tr>
<td><strong>Other Prominent Models</strong></td>
<td>StarCoder, OPT, Baichuan, Aquila, Qwen, InternLM, XGen, OpenLLaMA, Phi-2, Yi, OpenCodeInterpreter, Nous-Hermes, Gemma-it, Mistral-Instruct-v0.2-7B-Zeus,</td>
</tr>
</tbody>
</table>
<p><strong>Table 2: Some models supported by vLLM</strong></p>
<h2 id="get-started-in-model-garden">Get started in Model Garden<a class="headerlink" href="#get-started-in-model-garden" title="Permanent link">&para;</a></h2>
<p>The vLLM Cloud GPUs serving container is integrated into Model Garden the playground, one-click deployment, and Colab Enterprise notebook examples. This tutorial focuses on the Llama model family from Meta AI as an example.</p>
<h3 id="use-the-colab-enterprise-notebook">Use the Colab Enterprise notebook<a class="headerlink" href="#use-the-colab-enterprise-notebook" title="Permanent link">&para;</a></h3>
<p><a href="https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1">Playground</a> and <a href="https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1">one-click</a> deployments are also available but are not outlined in this tutorial.</p>
<ol>
<li>Navigate to the <a href="https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1">model card page</a> and click <strong>Open notebook</strong>.</li>
<li>Select the Vertex Serving notebook. The notebook is opened in Colab Enterprise.</li>
<li>Run through the notebook to deploy a model by using vLLM and send prediction requests to the endpoint.</li>
</ol>
<h2 id="setup-and-requirements">Setup and requirements<a class="headerlink" href="#setup-and-requirements" title="Permanent link">&para;</a></h2>
<p>This section outlines the necessary steps for setting up your Google Cloud project and ensuring you have the required resources for deploying and serving vLLM models.</p>
<h3 id="1-billing">1. Billing<a class="headerlink" href="#1-billing" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Enable Billing</strong>: Make sure that billing is enabled for your project. You can refer to <a href="https://cloud.google.com/billing/docs/how-to/modify-project">Enable, disable, or change billing for a project</a>.</li>
</ul>
<h3 id="2-gpu-availability-and-quotas">2. GPU availability and quotas<a class="headerlink" href="#2-gpu-availability-and-quotas" title="Permanent link">&para;</a></h3>
<ul>
<li>To run predictions using high-performance GPUs (NVIDIA A100 80GB or H100 80GB), make sure to check your quotas for these GPUs in your selected region:</li>
<li><a href="https://cloud.google.com/compute/docs/gpus">NVIDIA A100 80GB quota</a></li>
<li><a href="https://cloud.google.com/compute/docs/gpus">NVIDIA H100 80GB quota</a></li>
</ul>
<table>
<thead>
<tr>
<th>Machine Type</th>
<th>Accelerator Type</th>
<th>Recommended Regions</th>
</tr>
</thead>
<tbody>
<tr>
<td>a2-ultragpu-1g</td>
<td>1 NVIDIA_A100_80GB</td>
<td>us-central1, us-east4, europe-west4, asia-southeast1</td>
</tr>
<tr>
<td>a3-highgpu-8g</td>
<td>8 NVIDIA_H100_80GB</td>
<td>us-central1, us-west1, europe-west4, asia-southeast1</td>
</tr>
</tbody>
</table>
<h3 id="3-set-up-a-google-cloud-project">3. Set up a Google Cloud Project<a class="headerlink" href="#3-set-up-a-google-cloud-project" title="Permanent link">&para;</a></h3>
<p>Run the following code sample to make sure that your Google Cloud emvironment is correctly set up. This step installs necessary Python libraries and sets up access to Google Cloud resources. Actions include:</p>
<ul>
<li>Installation: Upgrade the <code>google-cloud-aiplatform</code> library and clone repository containing utility functions.</li>
<li>Environment Setup: Defining variables for the Google Cloud Project ID, region, and a unique Cloud Storage bucket for storing model artifacts.</li>
<li>API activation: Enable the Vertex AI amd Compute Engine APIs, which are essential for deploying and managing AI models.</li>
<li>Bucket configuration: Create a new Cloud Storage bucket or check an existing bucket to ensure it's in the correct region.</li>
<li>Vertex AI initialization: Initialize the Vertex AI client library with the project, location, and staging bucket settings.</li>
<li>Service account setup: Identify the default service account for running Vertex AI jobs and granting it the necessary permissions.</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">BUCKET_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://&quot;</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="n">REGION</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="err">!</span> <span class="n">pip3</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="o">--</span><span class="n">quiet</span> <span class="s1">&#39;google-cloud-aiplatform&gt;=1.64.0&#39;</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="err">!</span> <span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">GoogleCloudPlatform</span><span class="o">/</span><span class="n">vertex</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">samples</span><span class="o">.</span><span class="n">git</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="kn">import</span><span class="w"> </span><span class="nn">importlib</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="kn">from</span><span class="w"> </span><span class="nn">google.cloud</span><span class="w"> </span><span class="kn">import</span> <span class="n">aiplatform</span>
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="n">common_util</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span>
</span><span id="__span-0-18"><a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a> <span class="s2">&quot;vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util&quot;</span>
</span><span id="__span-0-19"><a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="p">)</span>
</span><span id="__span-0-20"><a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>
</span><span id="__span-0-21"><a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="n">models</span><span class="p">,</span> <span class="n">endpoints</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{}</span>
</span><span id="__span-0-22"><a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>
</span><span id="__span-0-23"><a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="n">PROJECT_ID</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;GOOGLE_CLOUD_PROJECT&quot;</span><span class="p">]</span>
</span><span id="__span-0-24"><a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>
</span><span id="__span-0-25"><a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="k">if</span> <span class="ow">not</span> <span class="n">REGION</span><span class="p">:</span>
</span><span id="__span-0-26"><a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a> <span class="n">REGION</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;GOOGLE_CLOUD_REGION&quot;</span><span class="p">]</span>
</span><span id="__span-0-27"><a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>
</span><span id="__span-0-28"><a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Enabling Vertex AI API and Compute Engine API.&quot;</span><span class="p">)</span>
</span><span id="__span-0-29"><a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a><span class="err">!</span> <span class="n">gcloud</span> <span class="n">services</span> <span class="n">enable</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span> <span class="n">compute</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span>
</span><span id="__span-0-30"><a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>
</span><span id="__span-0-31"><a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a><span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">%H%M%S&quot;</span><span class="p">)</span>
</span><span id="__span-0-32"><a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a><span class="n">BUCKET_NAME</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BUCKET_URI</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[:</span><span class="mi">3</span><span class="p">])</span>
</span><span id="__span-0-33"><a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>
</span><span id="__span-0-34"><a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a><span class="k">if</span> <span class="n">BUCKET_URI</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">BUCKET_URI</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span> <span class="ow">or</span> <span class="n">BUCKET_URI</span> <span class="o">==</span> <span class="s2">&quot;gs://&quot;</span><span class="p">:</span>
</span><span id="__span-0-35"><a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a> <span class="n">BUCKET_URI</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;gs://</span><span class="si">{</span><span class="n">PROJECT_ID</span><span class="si">}</span><span class="s2">-tmp-</span><span class="si">{</span><span class="n">now</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())[:</span><span class="mi">4</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-0-36"><a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a> <span class="n">BUCKET_NAME</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BUCKET_URI</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[:</span><span class="mi">3</span><span class="p">])</span>
</span><span id="__span-0-37"><a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a> <span class="err">!</span> <span class="n">gsutil</span> <span class="n">mb</span> <span class="o">-</span><span class="n">l</span> <span class="p">{</span><span class="n">REGION</span><span class="p">}</span> <span class="p">{</span><span class="n">BUCKET_URI</span><span class="p">}</span>
</span><span id="__span-0-38"><a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a><span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-39"><a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a> <span class="k">assert</span> <span class="n">BUCKET_URI</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">),</span> <span class="s2">&quot;BUCKET_URI must start with `gs://`.&quot;</span>
</span><span id="__span-0-40"><a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a> <span class="n">shell_output</span> <span class="o">=</span> <span class="err">!</span> <span class="n">gsutil</span> <span class="n">ls</span> <span class="o">-</span><span class="n">Lb</span> <span class="p">{</span><span class="n">BUCKET_NAME</span><span class="p">}</span> <span class="o">|</span> <span class="n">grep</span> <span class="s2">&quot;Location constraint:&quot;</span> <span class="o">|</span> <span class="n">sed</span> <span class="s2">&quot;s/Location constraint://&quot;</span>
</span><span id="__span-0-41"><a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a> <span class="n">bucket_region</span> <span class="o">=</span> <span class="n">shell_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</span><span id="__span-0-42"><a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a> <span class="k">if</span> <span class="n">bucket_region</span> <span class="o">!=</span> <span class="n">REGION</span><span class="p">:</span>
</span><span id="__span-0-43"><a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a> <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="__span-0-44"><a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a> <span class="s2">&quot;Bucket region </span><span class="si">%s</span><span class="s2"> is different from notebook region </span><span class="si">%s</span><span class="s2">&quot;</span>
</span><span id="__span-0-45"><a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a> <span class="o">%</span> <span class="p">(</span><span class="n">bucket_region</span><span class="p">,</span> <span class="n">REGION</span><span class="p">)</span>
</span><span id="__span-0-46"><a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a> <span class="p">)</span>
</span><span id="__span-0-47"><a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using this Bucket: </span><span class="si">{</span><span class="n">BUCKET_URI</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-0-48"><a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a>
</span><span id="__span-0-49"><a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a><span class="n">STAGING_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BUCKET_URI</span><span class="p">,</span> <span class="s2">&quot;temporal&quot;</span><span class="p">)</span>
</span><span id="__span-0-50"><a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a><span class="n">MODEL_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BUCKET_URI</span><span class="p">,</span> <span class="s2">&quot;llama3_1&quot;</span><span class="p">)</span>
</span><span id="__span-0-51"><a id="__codelineno-0-51" name="__codelineno-0-51" href="#__codelineno-0-51"></a>
</span><span id="__span-0-52"><a id="__codelineno-0-52" name="__codelineno-0-52" href="#__codelineno-0-52"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing Vertex AI API.&quot;</span><span class="p">)</span>
</span><span id="__span-0-53"><a id="__codelineno-0-53" name="__codelineno-0-53" href="#__codelineno-0-53"></a><span class="n">aiplatform</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">PROJECT_ID</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">REGION</span><span class="p">,</span> <span class="n">staging_bucket</span><span class="o">=</span><span class="n">STAGING_BUCKET</span><span class="p">)</span>
</span><span id="__span-0-54"><a id="__codelineno-0-54" name="__codelineno-0-54" href="#__codelineno-0-54"></a>
</span><span id="__span-0-55"><a id="__codelineno-0-55" name="__codelineno-0-55" href="#__codelineno-0-55"></a><span class="n">shell_output</span> <span class="o">=</span> <span class="err">!</span> <span class="n">gcloud</span> <span class="n">projects</span> <span class="n">describe</span> <span class="err">$</span><span class="n">PROJECT_ID</span>
</span><span id="__span-0-56"><a id="__codelineno-0-56" name="__codelineno-0-56" href="#__codelineno-0-56"></a><span class="n">project_number</span> <span class="o">=</span> <span class="n">shell_output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-0-57"><a id="__codelineno-0-57" name="__codelineno-0-57" href="#__codelineno-0-57"></a><span class="n">SERVICE_ACCOUNT</span> <span class="o">=</span> <span class="s2">&quot;your service account email&quot;</span>
</span><span id="__span-0-58"><a id="__codelineno-0-58" name="__codelineno-0-58" href="#__codelineno-0-58"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using this default Service Account:&quot;</span><span class="p">,</span> <span class="n">SERVICE_ACCOUNT</span><span class="p">)</span>
</span><span id="__span-0-59"><a id="__codelineno-0-59" name="__codelineno-0-59" href="#__codelineno-0-59"></a>
</span><span id="__span-0-60"><a id="__codelineno-0-60" name="__codelineno-0-60" href="#__codelineno-0-60"></a><span class="err">!</span> <span class="n">gsutil</span> <span class="n">iam</span> <span class="n">ch</span> <span class="n">serviceAccount</span><span class="p">:{</span><span class="n">SERVICE_ACCOUNT</span><span class="p">}:</span><span class="n">roles</span><span class="o">/</span><span class="n">storage</span><span class="o">.</span><span class="n">admin</span> <span class="err">$</span><span class="n">BUCKET_NAME</span>
</span><span id="__span-0-61"><a id="__codelineno-0-61" name="__codelineno-0-61" href="#__codelineno-0-61"></a>
</span><span id="__span-0-62"><a id="__codelineno-0-62" name="__codelineno-0-62" href="#__codelineno-0-62"></a><span class="err">!</span> <span class="n">gcloud</span> <span class="n">config</span> <span class="nb">set</span> <span class="n">project</span> <span class="err">$</span><span class="n">PROJECT_ID</span>
</span><span id="__span-0-63"><a id="__codelineno-0-63" name="__codelineno-0-63" href="#__codelineno-0-63"></a><span class="err">!</span> <span class="n">gcloud</span> <span class="n">projects</span> <span class="n">add</span><span class="o">-</span><span class="n">iam</span><span class="o">-</span><span class="n">policy</span><span class="o">-</span><span class="n">binding</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">output</span><span class="o">-</span><span class="n">enabled</span> <span class="p">{</span><span class="n">PROJECT_ID</span><span class="p">}</span> <span class="o">--</span><span class="n">member</span><span class="o">=</span><span class="n">serviceAccount</span><span class="p">:{</span><span class="n">SERVICE_ACCOUNT</span><span class="p">}</span> <span class="o">--</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;roles/storage.admin&quot;</span>
</span><span id="__span-0-64"><a id="__codelineno-0-64" name="__codelineno-0-64" href="#__codelineno-0-64"></a><span class="err">!</span> <span class="n">gcloud</span> <span class="n">projects</span> <span class="n">add</span><span class="o">-</span><span class="n">iam</span><span class="o">-</span><span class="n">policy</span><span class="o">-</span><span class="n">binding</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">output</span><span class="o">-</span><span class="n">enabled</span> <span class="p">{</span><span class="n">PROJECT_ID</span><span class="p">}</span> <span class="o">--</span><span class="n">member</span><span class="o">=</span><span class="n">serviceAccount</span><span class="p">:{</span><span class="n">SERVICE_ACCOUNT</span><span class="p">}</span> <span class="o">--</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;roles/aiplatform.user&quot;</span>
</span></code></pre></div>
<h2 id="using-hugging-face-with-meta-llama-31-32-and-vllm">Using Hugging Face with Meta Llama 3.1, 3.2, and vLLM<a class="headerlink" href="#using-hugging-face-with-meta-llama-31-32-and-vllm" title="Permanent link">&para;</a></h2>
<p>Meta's Llama 3.1 and 3.2 collections provide a range of multilingual large language models (LLMs) designed for high-quality text generation across various use cases. These models are pre-trained and instruction-tuned, excelling in tasks like multilingual dialogue, summarization, and agentic retrieval. Before using Llama 3.1 and 3.2 models, you must agree to their terms of use, as shown in the screenshot. The vLLM library offers an open-source streamlined serving environment with optimizations for latency, memory efficiency, and scalability.</p>
<p><strong>Important Note</strong>: Access to these models requires sharing your contact information and accepting the terms of use as outlined in the Meta Privacy Policy. Your request will then be reviewed by the repo's authors.</p>
<p><strong>Figure 1: Meta LLama 3 Community License Agreement</strong></p>
<h3 id="overview-of-meta-llama-31-and-32-collections">Overview of Meta Llama 3.1 and 3.2 Collections<a class="headerlink" href="#overview-of-meta-llama-31-and-32-collections" title="Permanent link">&para;</a></h3>
<p>The Llama 3.1 and 3.2 collections each cater to different deployment scales and model sizes, providing you with flexible options for multilingual dialogue tasks and beyond. Refer to the <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama#meta-models">Llama overview page</a> for more information.</p>
<ul>
<li><strong>Text-only</strong>: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in, text out).</li>
<li><strong>Vision and Vision Instruct</strong>: The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in, text out).
 Optimization: Like Llama 3.1, the 3.2 models are tailored for multilingual dialogue and perform well in retrieval and summarization tasks, achieving top results on standard benchmarks.</li>
<li><strong>Model Architecture</strong>: Llama 3.2 also features an auto-regressive transformer framework, with SFT and RLHF applied to align the models for helpfulness and safety.</li>
</ul>
<h3 id="hugging-face-user-access-tokens">Hugging Face user access tokens<a class="headerlink" href="#hugging-face-user-access-tokens" title="Permanent link">&para;</a></h3>
<p>This tutorial requires a read access token from the Hugging Face Hub to access the necessary resources. Follow these steps to set up your authentication:</p>
<p><strong>Figure 2: Hugging Face Access Token Settings</strong></p>
<ol>
<li>
<p>Generate a read access token:</p>
</li>
<li>
<p>Navigate to your <a href="https://huggingface.co/settings/tokens">Hugging Face account settings</a>.</p>
</li>
<li>Create a new token, assign it the Read role, and save the token securely.</li>
<li>
<p>Use the token:</p>
</li>
<li>
<p>Use the generated token to authenticate and access public or private repositories as needed for the tutorial.</p>
</li>
</ol>
<p><strong>Figure 3: Manage Hugging Face Access Token</strong></p>
<p>This setup ensures you have the appropriate level of access without unnecessary permissions. These practices enhance security and prevent accidental token exposure. For more information on setting up access tokens, visit Hugging Face Access Tokens <a href="https://huggingface.co/docs/hub/en/security-tokens">page</a>.</p>
<p>Avoid sharing or exposing your token publicly or online. When you set your token as an environment variable during deployment, it remains private to your project. Vertex AI ensures its security by preventing other users from accessing your models and endpoints.</p>
<p>For more information on protecting your access token, refer to the <a href="https://huggingface.co/docs/hub/en/security-tokens#best-practices">Hugging Face Access Tokens - Best Practices</a>.</p>
<hr />
<h2 id="deploying-text-only-llama-31-models-with-vllm">Deploying text-only Llama 3.1 Models with vLLM<a class="headerlink" href="#deploying-text-only-llama-31-models-with-vllm" title="Permanent link">&para;</a></h2>
<p>For production-level deployment of large language models, vLLM provides an efficient serving solution that optimizes memory usage, lowers latency, and increases throughput. This makes it particularly well-suited for handling the larger Llama 3.1 models as well as the multimodal Llama 3.2 models.</p>
<p><strong>Note:</strong> Recommended serving configurations: This example recommends using A100 80G or H100 GPUs for optimal serving efficiency and performance. These GPUs are now readily available and are the preferred options for deploying these models.</p>
<h3 id="step-1-choose-a-model-to-deploy">Step 1: Choose a model to deploy<a class="headerlink" href="#step-1-choose-a-model-to-deploy" title="Permanent link">&para;</a></h3>
<p>Choose the Llama 3.1 model variant to deploy. Available options include various sizes and instruction-tuned versions:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;Meta-Llama-3.1-8B&quot;</span> <span class="c1"># @param [&quot;Meta-Llama-3.1-8B&quot;, &quot;Meta-Llama-3.1-8B-Instruct&quot;, &quot;Meta-Llama-3.1-70B&quot;, &quot;Meta-Llama-3.1-70B-Instruct&quot;, &quot;Meta-Llama-3.1-405B-FP8&quot;, &quot;Meta-Llama-3.1-405B-Instruct-FP8&quot;]</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="n">hf_model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-Llama/&quot;</span> <span class="o">+</span> <span class="n">base_model_name</span>
</span></code></pre></div>
<h3 id="step-2-check-deployment-hardware-and-quota">Step 2: Check deployment hardware and quota<a class="headerlink" href="#step-2-check-deployment-hardware-and-quota" title="Permanent link">&para;</a></h3>
<p>The deploy function sets the appropriate GPU and machine type based on the model size and check the quota in that region for a particular project:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="k">if</span> <span class="s2">&quot;8b&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-12&quot;</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="k">elif</span> <span class="s2">&quot;70b&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-96&quot;</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="k">elif</span> <span class="s2">&quot;405b&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_H100_80GB&quot;</span>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;a3-highgpu-8g&quot;</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="k">else</span><span class="p">:</span>
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a> <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recommended GPU setting not found for: </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Verify GPU quota availability in your specified region:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="n">common_util</span><span class="o">.</span><span class="n">check_quota</span><span class="p">(</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a> <span class="n">project_id</span><span class="o">=</span><span class="n">PROJECT_ID</span><span class="p">,</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a> <span class="n">region</span><span class="o">=</span><span class="n">REGION</span><span class="p">,</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a> <span class="n">is_for_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="p">)</span>
</span></code></pre></div>
<h3 id="step-3-inspect-the-model-using-vllm">Step 3: Inspect the model using vLLM<a class="headerlink" href="#step-3-inspect-the-model-using-vllm" title="Permanent link">&para;</a></h3>
<p>The following function uploads the model to Vertex AI, configures deployment settings, and deploys it to an endpoint using vLLM.</p>
<ol>
<li><strong>Docker Image</strong>: The deployment uses a prebuilt vLLM Docker image for efficient serving.</li>
<li><strong>Configuration</strong>: Configure memory utilization, model length, and other vLLM settings. For more information on the arguments supported by the server, visit the official vLLM documentation page.</li>
<li><strong>Environment Variables</strong>: Set environment variables for authentication and deployment source.</li>
</ol>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">deploy_model_vllm</span><span class="p">(</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a> <span class="n">service_account</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a> <span class="n">base_model_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a> <span class="n">machine_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-8&quot;</span><span class="p">,</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a> <span class="n">accelerator_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span><span class="p">,</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a> <span class="n">accelerator_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a> <span class="n">gpu_memory_utilization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a> <span class="n">max_model_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a> <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a> <span class="n">enable_trust_remote_code</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a> <span class="n">enforce_eager</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-4-14"><a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a> <span class="n">enable_lora</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-4-15"><a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a> <span class="n">max_loras</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-4-16"><a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a> <span class="n">max_cpu_loras</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="__span-4-17"><a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a> <span class="n">use_dedicated_endpoint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-4-18"><a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a> <span class="n">max_num_seqs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
</span><span id="__span-4-19"><a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="p">]:</span>
</span><span id="__span-4-20"><a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a><span class="w"> </span><span class="sd">&quot;&quot;&quot;Deploys trained models with vLLM into Vertex AI.&quot;&quot;&quot;</span>
</span><span id="__span-4-21"><a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a> <span class="n">endpoint</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-4-22"><a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a> <span class="n">display_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">-endpoint&quot;</span><span class="p">,</span>
</span><span id="__span-4-23"><a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a> <span class="n">dedicated_endpoint_enabled</span><span class="o">=</span><span class="n">use_dedicated_endpoint</span><span class="p">,</span>
</span><span id="__span-4-24"><a id="__codelineno-4-24" name="__codelineno-4-24" href="#__codelineno-4-24"></a> <span class="p">)</span>
</span><span id="__span-4-25"><a id="__codelineno-4-25" name="__codelineno-4-25" href="#__codelineno-4-25"></a>
</span><span id="__span-4-26"><a id="__codelineno-4-26" name="__codelineno-4-26" href="#__codelineno-4-26"></a> <span class="k">if</span> <span class="s2">&quot;8b&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
</span><span id="__span-4-27"><a id="__codelineno-4-27" name="__codelineno-4-27" href="#__codelineno-4-27"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-4-28"><a id="__codelineno-4-28" name="__codelineno-4-28" href="#__codelineno-4-28"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-12&quot;</span>
</span><span id="__span-4-29"><a id="__codelineno-4-29" name="__codelineno-4-29" href="#__codelineno-4-29"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-4-30"><a id="__codelineno-4-30" name="__codelineno-4-30" href="#__codelineno-4-30"></a> <span class="k">elif</span> <span class="s2">&quot;70b&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
</span><span id="__span-4-31"><a id="__codelineno-4-31" name="__codelineno-4-31" href="#__codelineno-4-31"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-4-32"><a id="__codelineno-4-32" name="__codelineno-4-32" href="#__codelineno-4-32"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-96&quot;</span>
</span><span id="__span-4-33"><a id="__codelineno-4-33" name="__codelineno-4-33" href="#__codelineno-4-33"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="__span-4-34"><a id="__codelineno-4-34" name="__codelineno-4-34" href="#__codelineno-4-34"></a> <span class="k">elif</span> <span class="s2">&quot;405b&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
</span><span id="__span-4-35"><a id="__codelineno-4-35" name="__codelineno-4-35" href="#__codelineno-4-35"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_H100_80GB&quot;</span>
</span><span id="__span-4-36"><a id="__codelineno-4-36" name="__codelineno-4-36" href="#__codelineno-4-36"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;a3-highgpu-8g&quot;</span>
</span><span id="__span-4-37"><a id="__codelineno-4-37" name="__codelineno-4-37" href="#__codelineno-4-37"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="__span-4-38"><a id="__codelineno-4-38" name="__codelineno-4-38" href="#__codelineno-4-38"></a> <span class="k">else</span><span class="p">:</span>
</span><span id="__span-4-39"><a id="__codelineno-4-39" name="__codelineno-4-39" href="#__codelineno-4-39"></a> <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recommended GPU setting not found for: </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</span><span id="__span-4-40"><a id="__codelineno-4-40" name="__codelineno-4-40" href="#__codelineno-4-40"></a>
</span><span id="__span-4-41"><a id="__codelineno-4-41" name="__codelineno-4-41" href="#__codelineno-4-41"></a> <span class="n">common_util</span><span class="o">.</span><span class="n">check_quota</span><span class="p">(</span>
</span><span id="__span-4-42"><a id="__codelineno-4-42" name="__codelineno-4-42" href="#__codelineno-4-42"></a> <span class="n">project_id</span><span class="o">=</span><span class="n">PROJECT_ID</span><span class="p">,</span>
</span><span id="__span-4-43"><a id="__codelineno-4-43" name="__codelineno-4-43" href="#__codelineno-4-43"></a> <span class="n">region</span><span class="o">=</span><span class="n">REGION</span><span class="p">,</span>
</span><span id="__span-4-44"><a id="__codelineno-4-44" name="__codelineno-4-44" href="#__codelineno-4-44"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-4-45"><a id="__codelineno-4-45" name="__codelineno-4-45" href="#__codelineno-4-45"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-4-46"><a id="__codelineno-4-46" name="__codelineno-4-46" href="#__codelineno-4-46"></a> <span class="n">is_for_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="__span-4-47"><a id="__codelineno-4-47" name="__codelineno-4-47" href="#__codelineno-4-47"></a> <span class="p">)</span>
</span><span id="__span-4-48"><a id="__codelineno-4-48" name="__codelineno-4-48" href="#__codelineno-4-48"></a>
</span><span id="__span-4-49"><a id="__codelineno-4-49" name="__codelineno-4-49" href="#__codelineno-4-49"></a> <span class="n">vllm_args</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-4-50"><a id="__codelineno-4-50" name="__codelineno-4-50" href="#__codelineno-4-50"></a> <span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;vllm.entrypoints.api_server&quot;</span><span class="p">,</span> 
</span><span id="__span-4-51"><a id="__codelineno-4-51" name="__codelineno-4-51" href="#__codelineno-4-51"></a> <span class="s2">&quot;--host=0.0.0.0&quot;</span><span class="p">,</span> 
</span><span id="__span-4-52"><a id="__codelineno-4-52" name="__codelineno-4-52" href="#__codelineno-4-52"></a> <span class="s2">&quot;--port=8080&quot;</span><span class="p">,</span>
</span><span id="__span-4-53"><a id="__codelineno-4-53" name="__codelineno-4-53" href="#__codelineno-4-53"></a> <span class="sa">f</span><span class="s2">&quot;--model=</span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
</span><span id="__span-4-54"><a id="__codelineno-4-54" name="__codelineno-4-54" href="#__codelineno-4-54"></a> <span class="sa">f</span><span class="s2">&quot;--tensor-parallel-size=</span><span class="si">{</span><span class="n">accelerator_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-4-55"><a id="__codelineno-4-55" name="__codelineno-4-55" href="#__codelineno-4-55"></a> <span class="s2">&quot;--swap-space=16&quot;</span><span class="p">,</span>
</span><span id="__span-4-56"><a id="__codelineno-4-56" name="__codelineno-4-56" href="#__codelineno-4-56"></a> <span class="sa">f</span><span class="s2">&quot;--gpu-memory-utilization=</span><span class="si">{</span><span class="n">gpu_memory_utilization</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-4-57"><a id="__codelineno-4-57" name="__codelineno-4-57" href="#__codelineno-4-57"></a> <span class="sa">f</span><span class="s2">&quot;--max-model-len=</span><span class="si">{</span><span class="n">max_model_len</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;--dtype=</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-4-58"><a id="__codelineno-4-58" name="__codelineno-4-58" href="#__codelineno-4-58"></a> <span class="sa">f</span><span class="s2">&quot;--max-loras=</span><span class="si">{</span><span class="n">max_loras</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;--max-cpu-loras=</span><span class="si">{</span><span class="n">max_cpu_loras</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-4-59"><a id="__codelineno-4-59" name="__codelineno-4-59" href="#__codelineno-4-59"></a> <span class="sa">f</span><span class="s2">&quot;--max-num-seqs=</span><span class="si">{</span><span class="n">max_num_seqs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;--disable-log-stats&quot;</span>
</span><span id="__span-4-60"><a id="__codelineno-4-60" name="__codelineno-4-60" href="#__codelineno-4-60"></a> <span class="p">]</span>
</span><span id="__span-4-61"><a id="__codelineno-4-61" name="__codelineno-4-61" href="#__codelineno-4-61"></a>
</span><span id="__span-4-62"><a id="__codelineno-4-62" name="__codelineno-4-62" href="#__codelineno-4-62"></a> <span class="k">if</span> <span class="n">enable_trust_remote_code</span><span class="p">:</span>
</span><span id="__span-4-63"><a id="__codelineno-4-63" name="__codelineno-4-63" href="#__codelineno-4-63"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--trust-remote-code&quot;</span><span class="p">)</span>
</span><span id="__span-4-64"><a id="__codelineno-4-64" name="__codelineno-4-64" href="#__codelineno-4-64"></a> <span class="k">if</span> <span class="n">enforce_eager</span><span class="p">:</span>
</span><span id="__span-4-65"><a id="__codelineno-4-65" name="__codelineno-4-65" href="#__codelineno-4-65"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--enforce-eager&quot;</span><span class="p">)</span>
</span><span id="__span-4-66"><a id="__codelineno-4-66" name="__codelineno-4-66" href="#__codelineno-4-66"></a> <span class="k">if</span> <span class="n">enable_lora</span><span class="p">:</span>
</span><span id="__span-4-67"><a id="__codelineno-4-67" name="__codelineno-4-67" href="#__codelineno-4-67"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--enable-lora&quot;</span><span class="p">)</span>
</span><span id="__span-4-68"><a id="__codelineno-4-68" name="__codelineno-4-68" href="#__codelineno-4-68"></a> <span class="k">if</span> <span class="n">model_type</span><span class="p">:</span>
</span><span id="__span-4-69"><a id="__codelineno-4-69" name="__codelineno-4-69" href="#__codelineno-4-69"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--model-type=</span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-4-70"><a id="__codelineno-4-70" name="__codelineno-4-70" href="#__codelineno-4-70"></a>
</span><span id="__span-4-71"><a id="__codelineno-4-71" name="__codelineno-4-71" href="#__codelineno-4-71"></a> <span class="n">env_vars</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-4-72"><a id="__codelineno-4-72" name="__codelineno-4-72" href="#__codelineno-4-72"></a> <span class="s2">&quot;MODEL_ID&quot;</span><span class="p">:</span> <span class="n">model_id</span><span class="p">,</span>
</span><span id="__span-4-73"><a id="__codelineno-4-73" name="__codelineno-4-73" href="#__codelineno-4-73"></a> <span class="s2">&quot;DEPLOY_SOURCE&quot;</span><span class="p">:</span> <span class="s2">&quot;notebook&quot;</span><span class="p">,</span>
</span><span id="__span-4-74"><a id="__codelineno-4-74" name="__codelineno-4-74" href="#__codelineno-4-74"></a> <span class="s2">&quot;HF_TOKEN&quot;</span><span class="p">:</span> <span class="n">HF_TOKEN</span>
</span><span id="__span-4-75"><a id="__codelineno-4-75" name="__codelineno-4-75" href="#__codelineno-4-75"></a> <span class="p">}</span>
</span><span id="__span-4-76"><a id="__codelineno-4-76" name="__codelineno-4-76" href="#__codelineno-4-76"></a>
</span><span id="__span-4-77"><a id="__codelineno-4-77" name="__codelineno-4-77" href="#__codelineno-4-77"></a> <span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
</span><span id="__span-4-78"><a id="__codelineno-4-78" name="__codelineno-4-78" href="#__codelineno-4-78"></a> <span class="n">display_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="__span-4-79"><a id="__codelineno-4-79" name="__codelineno-4-79" href="#__codelineno-4-79"></a> <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">VLLM_DOCKER_URI</span><span class="p">,</span>
</span><span id="__span-4-80"><a id="__codelineno-4-80" name="__codelineno-4-80" href="#__codelineno-4-80"></a> <span class="n">serving_container_args</span><span class="o">=</span><span class="n">vllm_args</span><span class="p">,</span>
</span><span id="__span-4-81"><a id="__codelineno-4-81" name="__codelineno-4-81" href="#__codelineno-4-81"></a> <span class="n">serving_container_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">8080</span><span class="p">],</span>
</span><span id="__span-4-82"><a id="__codelineno-4-82" name="__codelineno-4-82" href="#__codelineno-4-82"></a> <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="s2">&quot;/generate&quot;</span><span class="p">,</span>
</span><span id="__span-4-83"><a id="__codelineno-4-83" name="__codelineno-4-83" href="#__codelineno-4-83"></a> <span class="n">serving_container_health_route</span><span class="o">=</span><span class="s2">&quot;/ping&quot;</span><span class="p">,</span>
</span><span id="__span-4-84"><a id="__codelineno-4-84" name="__codelineno-4-84" href="#__codelineno-4-84"></a> <span class="n">serving_container_environment_variables</span><span class="o">=</span><span class="n">env_vars</span><span class="p">,</span>
</span><span id="__span-4-85"><a id="__codelineno-4-85" name="__codelineno-4-85" href="#__codelineno-4-85"></a> <span class="n">serving_container_shared_memory_size_mb</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="__span-4-86"><a id="__codelineno-4-86" name="__codelineno-4-86" href="#__codelineno-4-86"></a> <span class="n">serving_container_deployment_timeout</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
</span><span id="__span-4-87"><a id="__codelineno-4-87" name="__codelineno-4-87" href="#__codelineno-4-87"></a> <span class="p">)</span>
</span><span id="__span-4-88"><a id="__codelineno-4-88" name="__codelineno-4-88" href="#__codelineno-4-88"></a> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deploying </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> on </span><span class="si">{</span><span class="n">machine_type</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">accelerator_count</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2"> GPU(s).&quot;</span><span class="p">)</span>
</span><span id="__span-4-89"><a id="__codelineno-4-89" name="__codelineno-4-89" href="#__codelineno-4-89"></a>
</span><span id="__span-4-90"><a id="__codelineno-4-90" name="__codelineno-4-90" href="#__codelineno-4-90"></a> <span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-4-91"><a id="__codelineno-4-91" name="__codelineno-4-91" href="#__codelineno-4-91"></a> <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
</span><span id="__span-4-92"><a id="__codelineno-4-92" name="__codelineno-4-92" href="#__codelineno-4-92"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-4-93"><a id="__codelineno-4-93" name="__codelineno-4-93" href="#__codelineno-4-93"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-4-94"><a id="__codelineno-4-94" name="__codelineno-4-94" href="#__codelineno-4-94"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-4-95"><a id="__codelineno-4-95" name="__codelineno-4-95" href="#__codelineno-4-95"></a> <span class="n">deploy_request_timeout</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span>
</span><span id="__span-4-96"><a id="__codelineno-4-96" name="__codelineno-4-96" href="#__codelineno-4-96"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">service_account</span><span class="p">,</span>
</span><span id="__span-4-97"><a id="__codelineno-4-97" name="__codelineno-4-97" href="#__codelineno-4-97"></a> <span class="p">)</span>
</span><span id="__span-4-98"><a id="__codelineno-4-98" name="__codelineno-4-98" href="#__codelineno-4-98"></a> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;endpoint_name:&quot;</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span><span id="__span-4-99"><a id="__codelineno-4-99" name="__codelineno-4-99" href="#__codelineno-4-99"></a>
</span><span id="__span-4-100"><a id="__codelineno-4-100" name="__codelineno-4-100" href="#__codelineno-4-100"></a> <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">endpoint</span>
</span></code></pre></div>
<h3 id="step-4-execute-deployment">Step 4: Execute deployment<a class="headerlink" href="#step-4-execute-deployment" title="Permanent link">&para;</a></h3>
<p>Run the deployment function with the selected model and configuration. This step deploys the model and returns the model and endpoint instances:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">HF_TOKEN</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">VLLM_DOCKER_URI</span> <span class="o">=</span> <span class="s2">&quot;us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20241001_0916_RC00&quot;</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="n">model_name</span> <span class="o">=</span> <span class="n">common_util</span><span class="o">.</span><span class="n">get_job_name_with_datetime</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">-serve-vllm&quot;</span><span class="p">)</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="mf">0.9</span>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="n">max_model_len</span> <span class="o">=</span> <span class="mi">4096</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="n">max_loras</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="n">models</span><span class="p">[</span><span class="s2">&quot;vllm_gpu&quot;</span><span class="p">],</span> <span class="n">endpoints</span><span class="p">[</span><span class="s2">&quot;vllm_gpu&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deploy_model_vllm</span><span class="p">(</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a> <span class="n">model_name</span><span class="o">=</span><span class="n">common_util</span><span class="o">.</span><span class="n">get_job_name_with_datetime</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">-serve&quot;</span><span class="p">),</span>
</span><span id="__span-5-12"><a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a> <span class="n">model_id</span><span class="o">=</span><span class="n">hf_model_id</span><span class="p">,</span>
</span><span id="__span-5-13"><a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">SERVICE_ACCOUNT</span><span class="p">,</span>
</span><span id="__span-5-14"><a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-5-15"><a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-5-16"><a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-5-17"><a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a> <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="n">gpu_memory_utilization</span><span class="p">,</span>
</span><span id="__span-5-18"><a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a> <span class="n">max_model_len</span><span class="o">=</span><span class="n">max_model_len</span><span class="p">,</span>
</span><span id="__span-5-19"><a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a> <span class="n">max_loras</span><span class="o">=</span><span class="n">max_loras</span><span class="p">,</span>
</span><span id="__span-5-20"><a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a> <span class="n">enforce_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-5-21"><a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a> <span class="n">enable_lora</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-5-22"><a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a> <span class="n">use_dedicated_endpoint</span><span class="o">=</span><span class="n">use_dedicated_endpoint</span><span class="p">,</span>
</span><span id="__span-5-23"><a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a><span class="p">)</span>
</span></code></pre></div>
<p>After running this code sample, your Llama 3.1 model will be deployed on Vertex AI and accessible through the specified endpoint. You can interact with it for inference tasks such as text generation, summarization, and dialogue. Depending on model size, new model deployment can take up to an hour. You can check the progress at Vertex Online Prediction.</p>
<p><strong>Figure 4: Llama 3.1 Deployment Endpoint in Vertex Dashboard</strong></p>
<hr />
<h2 id="making-predictions-with-llama-31-on-vertex-ai">Making predictions with Llama 3.1 on Vertex AI<a class="headerlink" href="#making-predictions-with-llama-31-on-vertex-ai" title="Permanent link">&para;</a></h2>
<p>After successfully deploying the Llama 3.1 model to Vertex AI, you can start making predictions by sending text prompts to the endpoint. This section provides an example of generating responses with various customizable parameters for controlling the output.</p>
<h3 id="step-1-define-your-prompt-and-parameters">Step 1: Define your prompt and parameters<a class="headerlink" href="#step-1-define-your-prompt-and-parameters" title="Permanent link">&para;</a></h3>
<p>Start by setting up your text prompt and sampling parameters to guide the model's response. Here are the key parameters:</p>
<ul>
<li><strong><code>prompt</code></strong>: The input text for which you want the model to generate a response. For example, prompt = "What is a car?".</li>
<li><strong><code>max_tokens</code></strong>: The maximum number of tokens in the generated output. Reducing this value can help prevent timeout issues.</li>
<li><strong><code>temperature</code></strong>: Controls the randomness of predictions. Higher values (for example, 1.0) increase diversity, while lower values (for example, 0.5) make the output more focused.</li>
<li><strong><code>top_p</code></strong>: Limits the sampling pool to the top cumulative probability. For example, setting top_p = 0.9 will only consider tokens within the top 90% probability mass.</li>
<li><strong><code>top_k</code></strong>: Limits sampling to the top k most likely tokens. For example, setting top_k = 50 will only sample from the top 50 tokens.</li>
<li><strong><code>raw_response</code></strong>: If True, returns the raw model output. If False, apply additional formatting with the structure "Prompt:\n{prompt}\nOutput:\n{output}".</li>
<li><strong><code>lora_id</code></strong> (optional): Path to LoRA weight files to apply Low-Rank Adaptation (LoRA) weights. This can be a Cloud Storage bucket or a Hugging Face repository URL. Note that this only works if <code>--enable-lora</code> is set in the deployment arguments. Dynamic LoRA is not supported for multimodal models.</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;What is a car?&quot;</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">50</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="n">top_p</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="n">top_k</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="n">raw_response</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="n">lora_id</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
</span></code></pre></div>
<h3 id="step-2-send-the-prediction-request">Step 2: Send the prediction request<a class="headerlink" href="#step-2-send-the-prediction-request" title="Permanent link">&para;</a></h3>
<p>Now that the instance is configured, you can send the prediction request to the deployed Vertex AI endpoint. This example shows how to make a prediction and print the result:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="n">response</span> <span class="o">=</span> <span class="n">endpoints</span><span class="p">[</span><span class="s2">&quot;vllm_gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a> <span class="n">instances</span><span class="o">=</span><span class="n">instances</span><span class="p">,</span> <span class="n">use_dedicated_endpoint</span><span class="o">=</span><span class="n">use_dedicated_endpoint</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="p">)</span>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="k">for</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">predictions</span><span class="p">:</span>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a> <span class="nb">print</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="example-output">Example output<a class="headerlink" href="#example-output" title="Permanent link">&para;</a></h3>
<p>Here's an example of how the model might respond to the prompt "What is a car?":</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">Human</span><span class="p">:</span> <span class="n">What</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">car</span><span class="err">?</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">Assistant</span><span class="p">:</span> <span class="n">A</span> <span class="n">car</span><span class="p">,</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">motor</span> <span class="n">car</span><span class="p">,</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">road</span><span class="o">-</span><span class="n">connected</span> <span class="n">human</span><span class="o">-</span><span class="n">transportation</span> <span class="n">system</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="n">used</span> <span class="n">to</span> <span class="n">move</span> <span class="n">people</span> <span class="ow">or</span> <span class="n">goods</span> <span class="kn">from</span><span class="w"> </span><span class="nn">one</span> <span class="n">place</span> <span class="n">to</span> <span class="n">another</span><span class="o">.</span>
</span></code></pre></div>
<h3 id="additional-notes">Additional notes<a class="headerlink" href="#additional-notes" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Moderation</strong>: To ensure safe content, you can moderate the generated text with Vertex AI's text moderation capabilities.</li>
<li><strong>Handling timeouts</strong>: If you encounter issues like <code>ServiceUnavailable: 503</code>, try reducing the <code>max_tokens</code> parameter.</li>
</ul>
<p>This approach provides a flexible way to interact with the Llama 3.1 model using different sampling techniques and LoRA adaptors, making it suitable for a variety of use cases from general-purpose text generation to task-specific responses.</p>
<hr />
<h2 id="deploying-multimodal-llama-32-models-with-vllm">Deploying multimodal Llama 3.2 models with vLLM<a class="headerlink" href="#deploying-multimodal-llama-32-models-with-vllm" title="Permanent link">&para;</a></h2>
<p>This section walks you through the process of uploading prebuilt Llama 3.2 models to the Model Registry and deploying them to a Vertex AI endpoint. The deployment time can take up to an hour, depending on the size of the model. Llama 3.2 models are available in multimodal versions that support both text and image inputs. vLLM supports:</p>
<ul>
<li>Text-only format</li>
<li>Single image + text format</li>
</ul>
<p>These formats make Llama 3.2 suitable for applications requiring both visual and text processing.</p>
<h3 id="step-1-choose-a-model-to-deploy_1">Step 1: Choose a model to deploy<a class="headerlink" href="#step-1-choose-a-model-to-deploy_1" title="Permanent link">&para;</a></h3>
<p>Specify the Llama 3.2 model variant you want to deploy. The following example uses <code>Llama-3.2-11B-Vision</code> as the selected model, but you can choose from other available options based on your requirements.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;Llama-3.2-11B-Vision&quot;</span> <span class="c1"># @param [&quot;Llama-3.2-1B&quot;, &quot;Llama-3.2-1B-Instruct&quot;, &quot;Llama-3.2-3B&quot;, &quot;Llama-3.2-3B-Instruct&quot;, &quot;Llama-3.2-11B-Vision&quot;, &quot;Llama-3.2-11B-Vision-Instruct&quot;, &quot;Llama-3.2-90B-Vision&quot;, &quot;Llama-3.2-90B-Vision-Instruct&quot;]</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="n">hf_model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-Llama/&quot;</span> <span class="o">+</span> <span class="n">base_model_name</span>
</span></code></pre></div>
<h3 id="step-2-configure-hardware-and-resources">Step 2: Configure hardware and resources<a class="headerlink" href="#step-2-configure-hardware-and-resources" title="Permanent link">&para;</a></h3>
<p>Select appropriate hardware for the model size. vLLM can use different GPUs depending on the computational needs of the model:</p>
<ul>
<li>1B and 3B models: Use NVIDIA L4 GPUs.</li>
<li>11B models: Use NVIDIA A100 GPUs.</li>
<li>90B models: Use NVIDIA H100 GPUs.</li>
</ul>
<p>This example configures the deployment based on the model selection:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="k">if</span> <span class="s2">&quot;3.2-1B&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span> <span class="ow">or</span> <span class="s2">&quot;3.2-3B&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="p">:</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-8&quot;</span>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a><span class="k">elif</span> <span class="s2">&quot;3.2-11B&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="p">:</span>
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_TESLA_A100&quot;</span>
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;a2-highgpu-1g&quot;</span>
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-10-9"><a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="k">elif</span> <span class="s2">&quot;3.2-90B&quot;</span> <span class="ow">in</span> <span class="n">base_model_name</span><span class="p">:</span>
</span><span id="__span-10-10"><a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a> <span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_H100_80GB&quot;</span>
</span><span id="__span-10-11"><a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a> <span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;a3-highgpu-8g&quot;</span>
</span><span id="__span-10-12"><a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a> <span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="__span-10-13"><a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a><span class="k">else</span><span class="p">:</span>
</span><span id="__span-10-14"><a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a> <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recommended GPU setting not found for: </span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Ensure that you have the required GPU quota:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="n">common_util</span><span class="o">.</span><span class="n">check_quota</span><span class="p">(</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a> <span class="n">project_id</span><span class="o">=</span><span class="n">PROJECT_ID</span><span class="p">,</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a> <span class="n">region</span><span class="o">=</span><span class="n">REGION</span><span class="p">,</span>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-11-5"><a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-11-6"><a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a> <span class="n">is_for_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="__span-11-7"><a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="p">)</span>
</span></code></pre></div>
<h3 id="step-3-deploy-the-model-using-vllm">Step 3: Deploy the model using vLLM<a class="headerlink" href="#step-3-deploy-the-model-using-vllm" title="Permanent link">&para;</a></h3>
<p>The following function handles the deployment of the Llama 3.2 model on Vertex AI. It configures the model's environment, memory utilization, and vLLM settings for efficient serving.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">deploy_model_vllm</span><span class="p">(</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a> <span class="n">service_account</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a> <span class="n">base_model_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a> <span class="n">machine_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-8&quot;</span><span class="p">,</span>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a> <span class="n">accelerator_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span><span class="p">,</span>
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a> <span class="n">accelerator_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a> <span class="n">gpu_memory_utilization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a> <span class="n">max_model_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a> <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span id="__span-12-12"><a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a> <span class="n">enable_trust_remote_code</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-12-13"><a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a> <span class="n">enforce_eager</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-12-14"><a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a> <span class="n">enable_lora</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-12-15"><a id="__codelineno-12-15" name="__codelineno-12-15" href="#__codelineno-12-15"></a> <span class="n">max_loras</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="__span-12-16"><a id="__codelineno-12-16" name="__codelineno-12-16" href="#__codelineno-12-16"></a> <span class="n">max_cpu_loras</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="__span-12-17"><a id="__codelineno-12-17" name="__codelineno-12-17" href="#__codelineno-12-17"></a> <span class="n">use_dedicated_endpoint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-12-18"><a id="__codelineno-12-18" name="__codelineno-12-18" href="#__codelineno-12-18"></a> <span class="n">max_num_seqs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
</span><span id="__span-12-19"><a id="__codelineno-12-19" name="__codelineno-12-19" href="#__codelineno-12-19"></a> <span class="n">model_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-12-20"><a id="__codelineno-12-20" name="__codelineno-12-20" href="#__codelineno-12-20"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="p">]:</span>
</span><span id="__span-12-21"><a id="__codelineno-12-21" name="__codelineno-12-21" href="#__codelineno-12-21"></a><span class="w"> </span><span class="sd">&quot;&quot;&quot;Deploys trained models with vLLM into Vertex AI.&quot;&quot;&quot;</span>
</span><span id="__span-12-22"><a id="__codelineno-12-22" name="__codelineno-12-22" href="#__codelineno-12-22"></a> <span class="n">endpoint</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-12-23"><a id="__codelineno-12-23" name="__codelineno-12-23" href="#__codelineno-12-23"></a> <span class="n">display_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">-endpoint&quot;</span><span class="p">,</span>
</span><span id="__span-12-24"><a id="__codelineno-12-24" name="__codelineno-12-24" href="#__codelineno-12-24"></a> <span class="n">dedicated_endpoint_enabled</span><span class="o">=</span><span class="n">use_dedicated_endpoint</span><span class="p">,</span>
</span><span id="__span-12-25"><a id="__codelineno-12-25" name="__codelineno-12-25" href="#__codelineno-12-25"></a> <span class="p">)</span>
</span><span id="__span-12-26"><a id="__codelineno-12-26" name="__codelineno-12-26" href="#__codelineno-12-26"></a>
</span><span id="__span-12-27"><a id="__codelineno-12-27" name="__codelineno-12-27" href="#__codelineno-12-27"></a> <span class="k">if</span> <span class="ow">not</span> <span class="n">base_model_id</span><span class="p">:</span>
</span><span id="__span-12-28"><a id="__codelineno-12-28" name="__codelineno-12-28" href="#__codelineno-12-28"></a> <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">model_id</span>
</span><span id="__span-12-29"><a id="__codelineno-12-29" name="__codelineno-12-29" href="#__codelineno-12-29"></a>
</span><span id="__span-12-30"><a id="__codelineno-12-30" name="__codelineno-12-30" href="#__codelineno-12-30"></a> <span class="n">vllm_args</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-12-31"><a id="__codelineno-12-31" name="__codelineno-12-31" href="#__codelineno-12-31"></a> <span class="s2">&quot;python&quot;</span><span class="p">,</span>
</span><span id="__span-12-32"><a id="__codelineno-12-32" name="__codelineno-12-32" href="#__codelineno-12-32"></a> <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
</span><span id="__span-12-33"><a id="__codelineno-12-33" name="__codelineno-12-33" href="#__codelineno-12-33"></a> <span class="s2">&quot;vllm.entrypoints.api_server&quot;</span><span class="p">,</span>
</span><span id="__span-12-34"><a id="__codelineno-12-34" name="__codelineno-12-34" href="#__codelineno-12-34"></a> <span class="s2">&quot;--host=0.0.0.0&quot;</span><span class="p">,</span>
</span><span id="__span-12-35"><a id="__codelineno-12-35" name="__codelineno-12-35" href="#__codelineno-12-35"></a> <span class="s2">&quot;--port=8080&quot;</span><span class="p">,</span>
</span><span id="__span-12-36"><a id="__codelineno-12-36" name="__codelineno-12-36" href="#__codelineno-12-36"></a> <span class="sa">f</span><span class="s2">&quot;--model=</span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-37"><a id="__codelineno-12-37" name="__codelineno-12-37" href="#__codelineno-12-37"></a> <span class="sa">f</span><span class="s2">&quot;--tensor-parallel-size=</span><span class="si">{</span><span class="n">accelerator_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-38"><a id="__codelineno-12-38" name="__codelineno-12-38" href="#__codelineno-12-38"></a> <span class="s2">&quot;--swap-space=16&quot;</span><span class="p">,</span>
</span><span id="__span-12-39"><a id="__codelineno-12-39" name="__codelineno-12-39" href="#__codelineno-12-39"></a> <span class="sa">f</span><span class="s2">&quot;--gpu-memory-utilization=</span><span class="si">{</span><span class="n">gpu_memory_utilization</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-40"><a id="__codelineno-12-40" name="__codelineno-12-40" href="#__codelineno-12-40"></a> <span class="sa">f</span><span class="s2">&quot;--max-model-len=</span><span class="si">{</span><span class="n">max_model_len</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-41"><a id="__codelineno-12-41" name="__codelineno-12-41" href="#__codelineno-12-41"></a> <span class="sa">f</span><span class="s2">&quot;--dtype=</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-42"><a id="__codelineno-12-42" name="__codelineno-12-42" href="#__codelineno-12-42"></a> <span class="sa">f</span><span class="s2">&quot;--max-loras=</span><span class="si">{</span><span class="n">max_loras</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-43"><a id="__codelineno-12-43" name="__codelineno-12-43" href="#__codelineno-12-43"></a> <span class="sa">f</span><span class="s2">&quot;--max-cpu-loras=</span><span class="si">{</span><span class="n">max_cpu_loras</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-44"><a id="__codelineno-12-44" name="__codelineno-12-44" href="#__codelineno-12-44"></a> <span class="sa">f</span><span class="s2">&quot;--max-num-seqs=</span><span class="si">{</span><span class="n">max_num_seqs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="__span-12-45"><a id="__codelineno-12-45" name="__codelineno-12-45" href="#__codelineno-12-45"></a> <span class="s2">&quot;--disable-log-stats&quot;</span><span class="p">,</span>
</span><span id="__span-12-46"><a id="__codelineno-12-46" name="__codelineno-12-46" href="#__codelineno-12-46"></a> <span class="p">]</span>
</span><span id="__span-12-47"><a id="__codelineno-12-47" name="__codelineno-12-47" href="#__codelineno-12-47"></a>
</span><span id="__span-12-48"><a id="__codelineno-12-48" name="__codelineno-12-48" href="#__codelineno-12-48"></a> <span class="k">if</span> <span class="n">enable_trust_remote_code</span><span class="p">:</span>
</span><span id="__span-12-49"><a id="__codelineno-12-49" name="__codelineno-12-49" href="#__codelineno-12-49"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--trust-remote-code&quot;</span><span class="p">)</span>
</span><span id="__span-12-50"><a id="__codelineno-12-50" name="__codelineno-12-50" href="#__codelineno-12-50"></a> <span class="k">if</span> <span class="n">enforce_eager</span><span class="p">:</span>
</span><span id="__span-12-51"><a id="__codelineno-12-51" name="__codelineno-12-51" href="#__codelineno-12-51"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--enforce-eager&quot;</span><span class="p">)</span>
</span><span id="__span-12-52"><a id="__codelineno-12-52" name="__codelineno-12-52" href="#__codelineno-12-52"></a> <span class="k">if</span> <span class="n">enable_lora</span><span class="p">:</span>
</span><span id="__span-12-53"><a id="__codelineno-12-53" name="__codelineno-12-53" href="#__codelineno-12-53"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--enable-lora&quot;</span><span class="p">)</span>
</span><span id="__span-12-54"><a id="__codelineno-12-54" name="__codelineno-12-54" href="#__codelineno-12-54"></a> <span class="k">if</span> <span class="n">model_type</span><span class="p">:</span>
</span><span id="__span-12-55"><a id="__codelineno-12-55" name="__codelineno-12-55" href="#__codelineno-12-55"></a> <span class="n">vllm_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--model-type=</span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-12-56"><a id="__codelineno-12-56" name="__codelineno-12-56" href="#__codelineno-12-56"></a>
</span><span id="__span-12-57"><a id="__codelineno-12-57" name="__codelineno-12-57" href="#__codelineno-12-57"></a> <span class="n">env_vars</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-12-58"><a id="__codelineno-12-58" name="__codelineno-12-58" href="#__codelineno-12-58"></a> <span class="s2">&quot;MODEL_ID&quot;</span><span class="p">:</span> <span class="n">base_model_id</span><span class="p">,</span>
</span><span id="__span-12-59"><a id="__codelineno-12-59" name="__codelineno-12-59" href="#__codelineno-12-59"></a> <span class="s2">&quot;DEPLOY_SOURCE&quot;</span><span class="p">:</span> <span class="s2">&quot;notebook&quot;</span><span class="p">,</span>
</span><span id="__span-12-60"><a id="__codelineno-12-60" name="__codelineno-12-60" href="#__codelineno-12-60"></a> <span class="p">}</span>
</span><span id="__span-12-61"><a id="__codelineno-12-61" name="__codelineno-12-61" href="#__codelineno-12-61"></a>
</span><span id="__span-12-62"><a id="__codelineno-12-62" name="__codelineno-12-62" href="#__codelineno-12-62"></a> <span class="c1"># HF_TOKEN is not a compulsory field and may not be defined.</span>
</span><span id="__span-12-63"><a id="__codelineno-12-63" name="__codelineno-12-63" href="#__codelineno-12-63"></a> <span class="k">try</span><span class="p">:</span>
</span><span id="__span-12-64"><a id="__codelineno-12-64" name="__codelineno-12-64" href="#__codelineno-12-64"></a> <span class="k">if</span> <span class="n">HF_TOKEN</span><span class="p">:</span>
</span><span id="__span-12-65"><a id="__codelineno-12-65" name="__codelineno-12-65" href="#__codelineno-12-65"></a> <span class="n">env_vars</span><span class="p">[</span><span class="s2">&quot;HF_TOKEN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">HF_TOKEN</span>
</span><span id="__span-12-66"><a id="__codelineno-12-66" name="__codelineno-12-66" href="#__codelineno-12-66"></a> <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
</span><span id="__span-12-67"><a id="__codelineno-12-67" name="__codelineno-12-67" href="#__codelineno-12-67"></a> <span class="k">pass</span>
</span><span id="__span-12-68"><a id="__codelineno-12-68" name="__codelineno-12-68" href="#__codelineno-12-68"></a>
</span><span id="__span-12-69"><a id="__codelineno-12-69" name="__codelineno-12-69" href="#__codelineno-12-69"></a> <span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
</span><span id="__span-12-70"><a id="__codelineno-12-70" name="__codelineno-12-70" href="#__codelineno-12-70"></a> <span class="n">display_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="__span-12-71"><a id="__codelineno-12-71" name="__codelineno-12-71" href="#__codelineno-12-71"></a> <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">VLLM_DOCKER_URI</span><span class="p">,</span>
</span><span id="__span-12-72"><a id="__codelineno-12-72" name="__codelineno-12-72" href="#__codelineno-12-72"></a> <span class="n">serving_container_args</span><span class="o">=</span><span class="n">vllm_args</span><span class="p">,</span>
</span><span id="__span-12-73"><a id="__codelineno-12-73" name="__codelineno-12-73" href="#__codelineno-12-73"></a> <span class="n">serving_container_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">8080</span><span class="p">],</span>
</span><span id="__span-12-74"><a id="__codelineno-12-74" name="__codelineno-12-74" href="#__codelineno-12-74"></a> <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="s2">&quot;/generate&quot;</span><span class="p">,</span>
</span><span id="__span-12-75"><a id="__codelineno-12-75" name="__codelineno-12-75" href="#__codelineno-12-75"></a> <span class="n">serving_container_health_route</span><span class="o">=</span><span class="s2">&quot;/ping&quot;</span><span class="p">,</span>
</span><span id="__span-12-76"><a id="__codelineno-12-76" name="__codelineno-12-76" href="#__codelineno-12-76"></a> <span class="n">serving_container_environment_variables</span><span class="o">=</span><span class="n">env_vars</span><span class="p">,</span>
</span><span id="__span-12-77"><a id="__codelineno-12-77" name="__codelineno-12-77" href="#__codelineno-12-77"></a> <span class="n">serving_container_shared_memory_size_mb</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">),</span>
</span><span id="__span-12-78"><a id="__codelineno-12-78" name="__codelineno-12-78" href="#__codelineno-12-78"></a> <span class="n">serving_container_deployment_timeout</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
</span><span id="__span-12-79"><a id="__codelineno-12-79" name="__codelineno-12-79" href="#__codelineno-12-79"></a> <span class="p">)</span>
</span><span id="__span-12-80"><a id="__codelineno-12-80" name="__codelineno-12-80" href="#__codelineno-12-80"></a> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deploying </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> on </span><span class="si">{</span><span class="n">machine_type</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">accelerator_count</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2"> GPU(s).&quot;</span><span class="p">)</span>
</span><span id="__span-12-81"><a id="__codelineno-12-81" name="__codelineno-12-81" href="#__codelineno-12-81"></a>
</span><span id="__span-12-82"><a id="__codelineno-12-82" name="__codelineno-12-82" href="#__codelineno-12-82"></a> <span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-12-83"><a id="__codelineno-12-83" name="__codelineno-12-83" href="#__codelineno-12-83"></a> <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
</span><span id="__span-12-84"><a id="__codelineno-12-84" name="__codelineno-12-84" href="#__codelineno-12-84"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-12-85"><a id="__codelineno-12-85" name="__codelineno-12-85" href="#__codelineno-12-85"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-12-86"><a id="__codelineno-12-86" name="__codelineno-12-86" href="#__codelineno-12-86"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-12-87"><a id="__codelineno-12-87" name="__codelineno-12-87" href="#__codelineno-12-87"></a> <span class="n">deploy_request_timeout</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span>
</span><span id="__span-12-88"><a id="__codelineno-12-88" name="__codelineno-12-88" href="#__codelineno-12-88"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">service_account</span><span class="p">,</span>
</span><span id="__span-12-89"><a id="__codelineno-12-89" name="__codelineno-12-89" href="#__codelineno-12-89"></a> <span class="p">)</span>
</span><span id="__span-12-90"><a id="__codelineno-12-90" name="__codelineno-12-90" href="#__codelineno-12-90"></a> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;endpoint_name:&quot;</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span><span id="__span-12-91"><a id="__codelineno-12-91" name="__codelineno-12-91" href="#__codelineno-12-91"></a>
</span><span id="__span-12-92"><a id="__codelineno-12-92" name="__codelineno-12-92" href="#__codelineno-12-92"></a> <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">endpoint</span>
</span></code></pre></div>
<h3 id="step-4-execute-deployment_1">Step 4: Execute deployment<a class="headerlink" href="#step-4-execute-deployment_1" title="Permanent link">&para;</a></h3>
<p>Run the deployment function with the configured model and settings. The function will return both the model and endpoint instances, which you can use for inference.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="n">model_name</span> <span class="o">=</span> <span class="n">common_util</span><span class="o">.</span><span class="n">get_job_name_with_datetime</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">-serve-vllm&quot;</span><span class="p">)</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="n">models</span><span class="p">[</span><span class="s2">&quot;vllm_gpu&quot;</span><span class="p">],</span> <span class="n">endpoints</span><span class="p">[</span><span class="s2">&quot;vllm_gpu&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deploy_model_vllm</span><span class="p">(</span>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a> <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a> <span class="n">model_id</span><span class="o">=</span><span class="n">hf_model_id</span><span class="p">,</span>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a> <span class="n">base_model_id</span><span class="o">=</span><span class="n">hf_model_id</span><span class="p">,</span>
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">SERVICE_ACCOUNT</span><span class="p">,</span>
</span><span id="__span-13-7"><a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a> <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
</span><span id="__span-13-8"><a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
</span><span id="__span-13-9"><a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
</span><span id="__span-13-10"><a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a> <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="n">gpu_memory_utilization</span><span class="p">,</span>
</span><span id="__span-13-11"><a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a> <span class="n">max_model_len</span><span class="o">=</span><span class="n">max_model_len</span><span class="p">,</span>
</span><span id="__span-13-12"><a id="__codelineno-13-12" name="__codelineno-13-12" href="#__codelineno-13-12"></a> <span class="n">enforce_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-13-13"><a id="__codelineno-13-13" name="__codelineno-13-13" href="#__codelineno-13-13"></a> <span class="n">use_dedicated_endpoint</span><span class="o">=</span><span class="n">use_dedicated_endpoint</span><span class="p">,</span>
</span><span id="__span-13-14"><a id="__codelineno-13-14" name="__codelineno-13-14" href="#__codelineno-13-14"></a> <span class="n">max_num_seqs</span><span class="o">=</span><span class="n">max_num_seqs</span><span class="p">,</span>
</span><span id="__span-13-15"><a id="__codelineno-13-15" name="__codelineno-13-15" href="#__codelineno-13-15"></a><span class="p">)</span>
</span></code></pre></div>
<p><strong>Figure 5: Llama 3.2 Deployment Endpoint in Vertex Dashboard</strong></p>
<p>Depending on model size, new model deployment can take up to an hour to complete. You can check its progress at Vertex Online Prediction.</p>
<h2 id="inference-with-vllm-on-vertex-ai-using-default-prediction-route">Inference with vLLM on Vertex AI using default prediction route<a class="headerlink" href="#inference-with-vllm-on-vertex-ai-using-default-prediction-route" title="Permanent link">&para;</a></h2>
<p>This section guides you through setting up inference for the Llama 3.2 Vision model on Vertex AI using the default prediction route. You'll use the vLLM library for efficient serving and interact with the model by sending a visual prompt in combination with text.</p>
<p>To get started, ensure your model endpoint is deployed and ready for predictions.</p>
<h3 id="step-1-define-your-prompt-and-parameters_1">Step 1: Define your prompt and parameters<a class="headerlink" href="#step-1-define-your-prompt-and-parameters_1" title="Permanent link">&para;</a></h3>
<p>This example provides an image URL and a text prompt, which the model will process to generate a response.</p>
<p><strong>Figure 6: Sample Image Input for prompting Llama 3.2</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="n">image_url</span> <span class="o">=</span> <span class="s2">&quot;https://images.pexels.com/photos/1254140/pexels-photo-1254140.jpeg&quot;</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">raw_prompt</span> <span class="o">=</span> <span class="s2">&quot;This is a picture of&quot;</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="c1"># Reference prompt formatting guidelines here: https://www.Llama.com/docs/model-cards-and-prompt-formats/Llama3_2/#-base-model-prompt</span>
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&lt;|begin_of_text|&gt;&lt;|image|&gt;</span><span class="si">{</span><span class="n">raw_prompt</span><span class="si">}</span><span class="s2">&quot;</span>
</span></code></pre></div>
<h3 id="step-2-configure-prediction-parameters">Step 2: Configure prediction parameters<a class="headerlink" href="#step-2-configure-prediction-parameters" title="Permanent link">&para;</a></h3>
<p>Adjust the following parameters to control the model's response:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">64</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a><span class="n">top_p</span> <span class="o">=</span> <span class="mf">0.95</span>
</span></code></pre></div>
<h3 id="step-3-prepare-the-prediction-request">Step 3: Prepare the prediction request<a class="headerlink" href="#step-3-prepare-the-prediction-request" title="Permanent link">&para;</a></h3>
<p>Set up the prediction request with the image URL, prompt, and other parameters.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="n">instances</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a> <span class="p">{</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a> <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a> <span class="s2">&quot;multi_modal_data&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">image_url</span><span class="p">},</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a> <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span><span class="p">,</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a> <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a> <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a> <span class="p">},</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="p">]</span>
</span></code></pre></div>
<h3 id="step-4-make-the-prediction">Step 4: Make the prediction<a class="headerlink" href="#step-4-make-the-prediction" title="Permanent link">&para;</a></h3>
<p>Send the request to your Vertex AI endpoint and process the response:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="n">response</span> <span class="o">=</span> <span class="n">endpoints</span><span class="p">[</span><span class="s2">&quot;vllm_gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">instances</span><span class="o">=</span><span class="n">instances</span><span class="p">)</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
</span><span id="__span-17-3"><a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="k">for</span> <span class="n">raw_prediction</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">predictions</span><span class="p">:</span>
</span><span id="__span-17-4"><a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a> <span class="n">prediction</span> <span class="o">=</span> <span class="n">raw_prediction</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;Output:&quot;</span><span class="p">)</span>
</span><span id="__span-17-5"><a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a> <span class="nb">print</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></code></pre></div>
<p>If you encounter a timeout issue (for example, <code>ServiceUnavailable: 503 Took too long to respond when processing</code>), try reducing the <code>max_tokens</code> value to a lower number, such as 20, to mitigate the response time.</p>
<hr />
<h2 id="inference-with-vllm-on-vertex-ai-using-openai-chat-completion">Inference with vLLM on Vertex AI using OpenAI Chat Completion<a class="headerlink" href="#inference-with-vllm-on-vertex-ai-using-openai-chat-completion" title="Permanent link">&para;</a></h2>
<p>This section covers how to perform inference on Llama 3.2 Vision models using the OpenAI Chat Completions API on Vertex AI. This approach lets you use multimodal capabilities by sending both images and text prompts to the model for more interactive responses.</p>
<h3 id="step-1-execute-deployment-of-llama-32-vision-instruct-model">Step 1: Execute deployment of Llama 3.2 Vision Instruct model<a class="headerlink" href="#step-1-execute-deployment-of-llama-32-vision-instruct-model" title="Permanent link">&para;</a></h3>
<p>Run the deployment function with the configured model and settings. The function will return both the model and endpoint instances, which you can use for inference.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;Llama-3.2-11B-Vision-Instruct&quot;</span>
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="n">hf_model_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;meta-llama/</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a><span class="n">model_name</span> <span class="o">=</span> <span class="n">common_util</span><span class="o">.</span><span class="n">get_job_name_with_datetime</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">-serve-vllm&quot;</span><span class="p">)</span>
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="n">model</span><span class="p">,</span> <span class="n">endpoint</span> <span class="o">=</span> <span class="n">deploy_model_vllm</span><span class="p">(</span>
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a> <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span>
</span><span id="__span-18-6"><a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a> <span class="n">model_id</span><span class="o">=</span><span class="n">hf_model_id</span><span class="p">,</span>
</span><span id="__span-18-7"><a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a> <span class="n">base_model_id</span><span class="o">=</span><span class="n">hf_model_id</span><span class="p">,</span>
</span><span id="__span-18-8"><a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a> <span class="n">service_account</span><span class="o">=</span><span class="n">SERVICE_ACCOUNT</span><span class="p">,</span>
</span><span id="__span-18-9"><a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a> <span class="n">machine_type</span><span class="o">=</span><span class="s2">&quot;a2-highgpu-1g&quot;</span><span class="p">,</span>
</span><span id="__span-18-10"><a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a> <span class="n">accelerator_type</span><span class="o">=</span><span class="s2">&quot;NVIDIA_TESLA_A100&quot;</span><span class="p">,</span>
</span><span id="__span-18-11"><a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a> <span class="n">accelerator_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-18-12"><a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a> <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
</span><span id="__span-18-13"><a id="__codelineno-18-13" name="__codelineno-18-13" href="#__codelineno-18-13"></a> <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
</span><span id="__span-18-14"><a id="__codelineno-18-14" name="__codelineno-18-14" href="#__codelineno-18-14"></a> <span class="n">enforce_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-18-15"><a id="__codelineno-18-15" name="__codelineno-18-15" href="#__codelineno-18-15"></a> <span class="n">max_num_seqs</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
</span><span id="__span-18-16"><a id="__codelineno-18-16" name="__codelineno-18-16" href="#__codelineno-18-16"></a><span class="p">)</span>
</span></code></pre></div>
<h3 id="step-2-configure-endpoint-resource">Step 2: Configure endpoint resource<a class="headerlink" href="#step-2-configure-endpoint-resource" title="Permanent link">&para;</a></h3>
<p>Begin by setting up the endpoint resource name for your Vertex AI deployment.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="n">ENDPOINT_RESOURCE_NAME</span> <span class="o">=</span> <span class="s2">&quot;projects/</span><span class="si">{}</span><span class="s2">/locations/</span><span class="si">{}</span><span class="s2">/endpoints/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a> <span class="n">PROJECT_ID</span><span class="p">,</span> <span class="n">REGION</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">.</span><span class="n">name</span>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="p">)</span>
</span></code></pre></div>
<h3 id="step-3-install-openai-sdk-and-authentication-libraries">Step 3: Install OpenAI SDK and authentication libraries<a class="headerlink" href="#step-3-install-openai-sdk-and-authentication-libraries" title="Permanent link">&para;</a></h3>
<p>To send requests using OpenAI's SDK, ensure the necessary libraries are installed:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">qU</span> <span class="n">openai</span> <span class="n">google</span><span class="o">-</span><span class="n">auth</span> <span class="n">requests</span>
</span></code></pre></div>
<h3 id="step-4-define-input-parameters-for-chat-completion">Step 4: Define input parameters for chat completion<a class="headerlink" href="#step-4-define-input-parameters-for-chat-completion" title="Permanent link">&para;</a></h3>
<p>Set up your image URL and text prompt that will be sent to the model. Adjust <code>max_tokens</code> and <code>temperature</code> to control the response length and randomness, respectively.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="n">user_image</span> <span class="o">=</span> <span class="s2">&quot;https://images.freeimages.com/images/large-previews/ab3/puppy-2-1404644.jpg&quot;</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a><span class="n">user_message</span> <span class="o">=</span> <span class="s2">&quot;Describe this image?&quot;</span>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a><span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">50</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a><span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span>
</span></code></pre></div>
<h3 id="step-5-set-up-authentication-and-base-url">Step 5: Set up authentication and base URL<a class="headerlink" href="#step-5-set-up-authentication-and-base-url" title="Permanent link">&para;</a></h3>
<p>Retrieve your credentials and set the base URL for API requests.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">google.auth</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="n">creds</span><span class="p">,</span> <span class="n">project</span> <span class="o">=</span> <span class="n">google</span><span class="o">.</span><span class="n">auth</span><span class="o">.</span><span class="n">default</span><span class="p">()</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="n">auth_req</span> <span class="o">=</span> <span class="n">google</span><span class="o">.</span><span class="n">auth</span><span class="o">.</span><span class="n">transport</span><span class="o">.</span><span class="n">requests</span><span class="o">.</span><span class="n">Request</span><span class="p">()</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="n">creds</span><span class="o">.</span><span class="n">refresh</span><span class="p">(</span><span class="n">auth_req</span><span class="p">)</span>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="n">BASE_URL</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a> <span class="sa">f</span><span class="s2">&quot;https://</span><span class="si">{</span><span class="n">REGION</span><span class="si">}</span><span class="s2">-aiplatform.googleapis.com/v1beta1/</span><span class="si">{</span><span class="n">ENDPOINT_RESOURCE_NAME</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a><span class="p">)</span>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a><span class="k">try</span><span class="p">:</span>
</span><span id="__span-22-12"><a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a> <span class="k">if</span> <span class="n">use_dedicated_endpoint</span><span class="p">:</span>
</span><span id="__span-22-13"><a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a> <span class="n">BASE_URL</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;https://</span><span class="si">{</span><span class="n">DEDICATED_ENDPOINT_DNS</span><span class="si">}</span><span class="s2">/v1beta1/</span><span class="si">{</span><span class="n">ENDPOINT_RESOURCE_NAME</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-22-14"><a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a><span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
</span><span id="__span-22-15"><a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a> <span class="k">pass</span>
</span></code></pre></div>
<h3 id="step-6-send-chat-completion-request">Step 6: Send Chat Completion request<a class="headerlink" href="#step-6-send-chat-completion-request" title="Permanent link">&para;</a></h3>
<p>Using OpenAI's Chat Completions API, send the image and text prompt to your Vertex AI endpoint:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">BASE_URL</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">creds</span><span class="o">.</span><span class="n">token</span><span class="p">)</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>
</span><span id="__span-23-3"><a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a><span class="n">model_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-23-4"><a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
</span><span id="__span-23-5"><a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-23-6"><a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a> <span class="p">{</span>
</span><span id="__span-23-7"><a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
</span><span id="__span-23-8"><a id="__codelineno-23-8" name="__codelineno-23-8" href="#__codelineno-23-8"></a> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-23-9"><a id="__codelineno-23-9" name="__codelineno-23-9" href="#__codelineno-23-9"></a> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image_url&quot;</span><span class="p">,</span> <span class="s2">&quot;image_url&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="n">user_image</span><span class="p">}},</span>
</span><span id="__span-23-10"><a id="__codelineno-23-10" name="__codelineno-23-10" href="#__codelineno-23-10"></a> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">user_message</span><span class="p">},</span>
</span><span id="__span-23-11"><a id="__codelineno-23-11" name="__codelineno-23-11" href="#__codelineno-23-11"></a> <span class="p">],</span>
</span><span id="__span-23-12"><a id="__codelineno-23-12" name="__codelineno-23-12" href="#__codelineno-23-12"></a> <span class="p">}</span>
</span><span id="__span-23-13"><a id="__codelineno-23-13" name="__codelineno-23-13" href="#__codelineno-23-13"></a> <span class="p">],</span>
</span><span id="__span-23-14"><a id="__codelineno-23-14" name="__codelineno-23-14" href="#__codelineno-23-14"></a> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
</span><span id="__span-23-15"><a id="__codelineno-23-15" name="__codelineno-23-15" href="#__codelineno-23-15"></a> <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
</span><span id="__span-23-16"><a id="__codelineno-23-16" name="__codelineno-23-16" href="#__codelineno-23-16"></a><span class="p">)</span>
</span><span id="__span-23-17"><a id="__codelineno-23-17" name="__codelineno-23-17" href="#__codelineno-23-17"></a>
</span><span id="__span-23-18"><a id="__codelineno-23-18" name="__codelineno-23-18" href="#__codelineno-23-18"></a><span class="nb">print</span><span class="p">(</span><span class="n">model_response</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="optional-step-7-reconnect-to-an-existing-endpoint">(Optional ) Step 7: Reconnect to an existing endpoint<a class="headerlink" href="#optional-step-7-reconnect-to-an-existing-endpoint" title="Permanent link">&para;</a></h3>
<p>To reconnect to a previously created endpoint, use the endpoint ID. This step is useful if you want to reuse an endpoint instead of creating a new one.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="n">endpoint_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
</span><span id="__span-24-2"><a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a><span class="n">aip_endpoint_name</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="__span-24-3"><a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a> <span class="sa">f</span><span class="s2">&quot;projects/</span><span class="si">{</span><span class="n">PROJECT_ID</span><span class="si">}</span><span class="s2">/locations/</span><span class="si">{</span><span class="n">REGION</span><span class="si">}</span><span class="s2">/endpoints/</span><span class="si">{</span><span class="n">endpoint_name</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-24-4"><a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a><span class="p">)</span>
</span><span id="__span-24-5"><a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a><span class="n">endpoint</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="p">(</span><span class="n">aip_endpoint_name</span><span class="p">)</span>
</span></code></pre></div>
<p>This setup provides flexibility to switch between newly created and existing endpoints as needed, allowing for streamlined testing and deployment.</p>
<h2 id="cleanup">Cleanup<a class="headerlink" href="#cleanup" title="Permanent link">&para;</a></h2>
<p>To avoid ongoing charges and free up resources, make sure to delete the deployed models, endpoints, and optionally the storage bucket used for this experiment.</p>
<h3 id="step-1-delete-endpoints-and-models">Step 1: Delete Endpoints and Models<a class="headerlink" href="#step-1-delete-endpoints-and-models" title="Permanent link">&para;</a></h3>
<p>The following code will undeploy each model and delete the associated endpoints:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="c1"># Undeploy model and delete endpoint</span>
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a><span class="k">for</span> <span class="n">endpoint</span> <span class="ow">in</span> <span class="n">endpoints</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a> <span class="n">endpoint</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-25-4"><a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>
</span><span id="__span-25-5"><a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a><span class="c1"># Delete models</span>
</span><span id="__span-25-6"><a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
</span><span id="__span-25-7"><a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a> <span class="n">model</span><span class="o">.</span><span class="n">delete</span><span class="p">()</span>
</span></code></pre></div>
<h3 id="step-2-optional-delete-cloud-storage-bucket">Step 2: (Optional) Delete Cloud Storage Bucket<a class="headerlink" href="#step-2-optional-delete-cloud-storage-bucket" title="Permanent link">&para;</a></h3>
<p>If you created a Cloud Storage bucket specifically for this experiment, you can delete it by setting delete_bucket to True. This step is optional but recommended if the bucket is no longer needed.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="n">delete_bucket</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a><span class="k">if</span> <span class="n">delete_bucket</span><span class="p">:</span>
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a> <span class="err">!</span> <span class="n">gsutil</span> <span class="o">-</span><span class="n">m</span> <span class="n">rm</span> <span class="o">-</span><span class="n">r</span> <span class="err">$</span><span class="n">BUCKET_NAME</span>
</span></code></pre></div>
<p>By following these steps, you ensure that all resources used in this tutorial are cleaned up, reducing any unnecessary costs associated with the experiment.</p>
<hr />
<h2 id="debugging-common-issues">Debugging common issues<a class="headerlink" href="#debugging-common-issues" title="Permanent link">&para;</a></h2>
<p>This section provides guidance on identifying and resolving common issues encountered during vLLM model deployment and inference on Vertex AI.</p>
<h3 id="check-the-logs">Check the logs<a class="headerlink" href="#check-the-logs" title="Permanent link">&para;</a></h3>
<p>Check the logs to identify the root cause of deployment failures or unexpected behavior:</p>
<ol>
<li><strong>Navigate to Vertex AI Prediction Console:</strong> Go to the <a href="https://console.cloud.google.com/vertex-ai/online-prediction/endpoints">Vertex AI Prediction Console</a> in the Google Cloud console.</li>
<li><strong>Select the Endpoint:</strong> Click the endpoint experiencing issues. The status should indicate if the deployment has failed.</li>
<li><strong>View Logs:</strong> Click the endpoint and then navigate to the <strong>Logs</strong> tab or click <strong>View logs</strong>. This directs you to Cloud Logging, filtered to show logs specific to that endpoint and model deployment. You can also access logs through the Cloud Logging service directly.</li>
<li><strong>Analyze the Logs:</strong> Review the log entries for error messages, warnings, and other relevant information. View timestamps to correlate log entries with specific actions. Look for issues around resource constraints (memory and CPU), authentication problems, or configuration errors.</li>
</ol>
<h3 id="common-issue-1-cuda-out-of-memory-oom-during-deployment">Common Issue 1: CUDA Out of Memory (OOM) during deployment<a class="headerlink" href="#common-issue-1-cuda-out-of-memory-oom-during-deployment" title="Permanent link">&para;</a></h3>
<p>CUDA Out of Memory (OOM) errors occur when the model's memory usage exceeds the available GPU capacity.</p>
<p>In the case of the text only model, we used the following engine arguments:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;Meta-Llama-3.1-8B&quot;</span>
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a><span class="n">hf_model_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;meta-llama/</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a><span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-27-4"><a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a><span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-27-5"><a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a><span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-12&quot;</span>
</span><span id="__span-27-6"><a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a><span class="n">accelerator_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-27-7"><a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a><span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="mf">0.9</span>
</span><span id="__span-27-8"><a id="__codelineno-27-8" name="__codelineno-27-8" href="#__codelineno-27-8"></a><span class="n">max_model_len</span> <span class="o">=</span> <span class="mi">4096</span>
</span><span id="__span-27-9"><a id="__codelineno-27-9" name="__codelineno-27-9" href="#__codelineno-27-9"></a><span class="n">dtype</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span>
</span><span id="__span-27-10"><a id="__codelineno-27-10" name="__codelineno-27-10" href="#__codelineno-27-10"></a><span class="n">max_num_seqs</span> <span class="o">=</span> <span class="mi">256</span>
</span></code></pre></div>
<p>In the case of the multimodal model, we used the following engine arguments:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="n">base_model_name</span> <span class="o">=</span> <span class="s2">&quot;Llama-3.2-11B-Vision-Instruct&quot;</span>
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a><span class="n">hf_model_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;meta-llama/</span><span class="si">{</span><span class="n">base_model_name</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a><span class="n">accelerator_type</span> <span class="o">=</span> <span class="s2">&quot;NVIDIA_L4&quot;</span>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a><span class="n">accelerator_count</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a><span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;g2-standard-12&quot;</span>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a><span class="n">accelerator_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="mf">0.9</span>
</span><span id="__span-28-8"><a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a><span class="n">max_model_len</span> <span class="o">=</span> <span class="mi">4096</span>
</span><span id="__span-28-9"><a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a><span class="n">dtype</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span>
</span><span id="__span-28-10"><a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a><span class="n">max_num_seqs</span> <span class="o">=</span> <span class="mi">12</span>
</span></code></pre></div>
<p>Deploying the multimodal model with max_num_seqs = 256, like we did in the case of text only model could cause the following error:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a><span class="p">[</span><span class="n">rank0</span><span class="p">]:</span> <span class="n">torch</span><span class="o">.</span><span class="n">OutOfMemoryError</span><span class="p">:</span> <span class="n">CUDA</span> <span class="n">out</span> <span class="n">of</span> <span class="n">memory</span><span class="o">.</span> <span class="n">Tried</span> <span class="n">to</span> <span class="n">allocate</span> <span class="mf">3.91</span> <span class="n">GiB</span><span class="o">.</span> <span class="n">GPU</span> <span class="mi">0</span> <span class="n">has</span> <span class="n">a</span> <span class="n">total</span> <span class="n">capacity</span> <span class="n">of</span> <span class="mf">39.38</span> <span class="n">GiB</span> <span class="n">of</span> <span class="n">which</span> <span class="mf">3.76</span> <span class="n">GiB</span> <span class="ow">is</span> <span class="n">free</span><span class="o">.</span> <span class="n">Including</span> <span class="n">non</span><span class="o">-</span><span class="n">PyTorch</span> <span class="n">memory</span><span class="p">,</span> <span class="n">this</span> <span class="n">process</span> <span class="n">has</span> <span class="mi">0</span> <span class="nb">bytes</span> <span class="n">memory</span> <span class="ow">in</span> <span class="n">use</span><span class="o">.</span> <span class="n">Of</span> <span class="n">the</span> <span class="n">allocated</span> <span class="n">memory</span> <span class="mf">34.94</span> <span class="n">GiB</span> <span class="ow">is</span> <span class="n">allocated</span> <span class="n">by</span> <span class="n">PyTorch</span><span class="p">,</span> <span class="ow">and</span> <span class="mf">175.15</span> <span class="n">MiB</span> <span class="ow">is</span> <span class="n">reserved</span> <span class="n">by</span> <span class="n">PyTorch</span> <span class="n">but</span> <span class="n">unallocated</span><span class="o">.</span> <span class="n">If</span> <span class="n">reserved</span> <span class="n">but</span> <span class="n">unallocated</span> <span class="n">memory</span> <span class="ow">is</span> <span class="n">large</span> <span class="k">try</span> <span class="n">setting</span> <span class="n">PYTORCH_CUDA_ALLOC_CONF</span><span class="o">=</span><span class="n">expandable_segments</span><span class="p">:</span><span class="kc">True</span> <span class="n">to</span> <span class="n">avoid</span> <span class="n">fragmentation</span><span class="o">.</span> <span class="n">See</span> <span class="n">documentation</span> <span class="k">for</span> <span class="n">Memory</span> <span class="n">Management</span> <span class="p">(</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">pytorch</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">docs</span><span class="o">/</span><span class="n">stable</span><span class="o">/</span><span class="n">notes</span><span class="o">/</span><span class="n">cuda</span><span class="o">.</span><span class="n">html</span><span class="c1">#environment-variables)</span>
</span></code></pre></div>
<p><strong>Figure 7: Out of Memory (OOM) GPU Error Log</strong></p>
<p>Understand <code>max_num_seqs</code> and GPU Memory:</p>
<ul>
<li>The <code>max_num_seqs</code> parameter defines the maximum number of concurrent requests the model can handle.</li>
<li>Each sequence processed by the model consumes GPU memory. The total memory usage is proportional to <code>max_num_seqs</code> times the memory per sequence.</li>
<li>Text-only models (like Meta-Llama-3.1-8B) generally consume less memory per sequence than multimodal models (like Llama-3.2-11B-Vision-Instruct), which process both text and images.</li>
</ul>
<p>Review the Error Log (figure 8):</p>
<ul>
<li>The log shows a <code>torch.OutOfMemoryError</code> when trying to allocate memory on the GPU.</li>
<li>The error occurs because the model's memory usage exceeds the available GPU capacity. The NVIDIA L4 GPU has 24 GB, and setting the <code>max_num_seqs</code> parameter too high for the multimodal model causes an overflow.</li>
<li>The log suggests setting <code>PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True</code> to improve memory management, though the primary issue here is high memory usage.</li>
</ul>
<p><strong>Figure 8: Failed Llama 3.2 Deployment</strong></p>
<p><strong>Figure 9: Model Version Details Panel</strong></p>
<p>To resolve this issue, navigate to the <a href="https://console.cloud.google.com/vertex-ai/online-prediction/endpoints">Vertex AI Prediction Console</a>, click the endpoint. The status should indicate that the deployment has failed. Click to view the logs. Verify that max-num-seqs = 256. This value is too high for Llama-3.2-11B-Vision-Instruct. A more adequate value should be 12.</p>
<h3 id="common-issue-2-hugging-face-token-needed">Common Issue 2: Hugging Face token needed<a class="headerlink" href="#common-issue-2-hugging-face-token-needed" title="Permanent link">&para;</a></h3>
<p>Hugging Face token errors occur when the model is gated and requires proper authentication credentials to be accessed.</p>
<p>The following screenshot displays a log entry in Google Cloud's Log Explorer showing an error message related to accessing the Meta LLaMA-3.2-11B-Vision model hosted on Hugging Face. The error indicates that access to the model is restricted, requiring authentication to proceed. The message specifically states, "Cannot access gated repository for URL," highlighting that the model is gated and requires proper authentication credentials to be accessed. This log entry can help troubleshoot authentication issues when working with restricted resources in external repositories.</p>
<p><strong>Figure 10: Hugging Face Token Error</strong></p>
<p>To resolve this issue, verify the permissions of your Hugging Face access token. Copy the latest token and deploy a new endpoint.</p>
<h3 id="common-issue-3-chat-template-needed">Common Issue 3: Chat template needed<a class="headerlink" href="#common-issue-3-chat-template-needed" title="Permanent link">&para;</a></h3>
<p>Chat template errors occur when the default chat template is no longer allowed, and a custom chat template must be provided if the tokenizer does not define one.</p>
<p>This screenshot shows a log entry in Google Cloud's Log Explorer, where a ValueError occurs due to a missing chat template in the transformers library version 4.44. The error message indicates that the default chat template is no longer allowed, and a custom chat template must be provided if the tokenizer does not define one. This error highlights a recent change in the library requiring explicit definition of a chat template, useful for debugging issues when deploying chat-based applications.</p>
<p><strong>Figure 11: Chat Template Needed</strong></p>
<p>To bypass this, make sure to provide a chat template during deployment using the <code>--chat-template</code> input argument. Sample templates can be found in the <a href="https://github.com/vllm-project/vllm/tree/main/examples">vLLM examples repository</a>.</p>
<h3 id="common-issue-4-model-max-seq-len">Common Issue 4: Model Max Seq Len<a class="headerlink" href="#common-issue-4-model-max-seq-len" title="Permanent link">&para;</a></h3>
<p>Model max sequence length errors occur when the model's max seq len (4096) is larger than the maximum number of tokens that can be stored in KV cache (2256).</p>
<p><strong>Figure 12: Max Seq Length too Large</strong></p>
<p>ValueError: The model's max seq len (4096) is larger than the maximum number of tokens that can be stored in KV cache (2256). Try increasing <code>gpu_memory_utilization</code> or decreasing <code>max_model_len</code> when initializing the engine.</p>
<p>To resolve this problem, set max_model_len 2048, which is less than 2256. Another resolution for this issue is to use more or larger GPUs. tensor-parallel-size will need to be set appropriately if opting to use more GPUs.</p>







  
  






                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.action.edit", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.progress", "navigation.path", "navigation.top", "navigation.tracking", "toc.follow", "navigation.tabs", "navigation.sections", "navigation.tracking", "navigation.top", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../js/chat-widget/gemini-client.js"></script>
      
        <script src="../../../js/chat-widget/init.js"></script>
      
        <script src="../../../js/chat-widget/setup-key.js"></script>
      
        <script src="../../../js/chat-widget-helper.js"></script>
      
    
  </body>
</html>